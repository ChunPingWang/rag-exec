{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 範例 11：通用聊天程式\n",
    "\n",
    "可以在 Ollama 和 LM Studio 之間切換的聊天機器人！\n",
    "\n",
    "## 學習目標\n",
    "- 建立支援多種後端的聊天程式\n",
    "- 學會抽象化 API 差異\n",
    "- 實作靈活的程式架構\n",
    "\n",
    "## 為什麼需要通用程式？\n",
    "- 可以根據需求選擇不同的 AI 後端\n",
    "- 方便比較不同模型的效果\n",
    "- 程式碼更容易維護和擴展\n",
    "\n",
    "## 前置需求\n",
    "- Ollama 或 LM Studio 運行中\n",
    "- 安裝 openai 套件：`pip install openai`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: 匯入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: 建立通用聊天機器人類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalChatBot:\n",
    "    \"\"\"\n",
    "    通用聊天機器人\n",
    "    支援 Ollama 和 LM Studio 兩種後端\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backend=\"lmstudio\", model=None):\n",
    "        \"\"\"\n",
    "        初始化聊天機器人\n",
    "\n",
    "        參數：\n",
    "            backend: \"ollama\" 或 \"lmstudio\"\n",
    "            model: 模型名稱（可選）\n",
    "        \"\"\"\n",
    "        self.backend = backend\n",
    "        self.messages = []\n",
    "\n",
    "        if backend == \"lmstudio\":\n",
    "            self.client = OpenAI(\n",
    "                base_url=\"http://localhost:1234/v1\",\n",
    "                api_key=\"not-needed\"\n",
    "            )\n",
    "            self.model = model or \"gpt-oss-120b\"\n",
    "        elif backend == \"ollama\":\n",
    "            self.url = \"http://localhost:11434/api/chat\"\n",
    "            self.model = model or \"gpt-oss:120b\"\n",
    "        else:\n",
    "            raise ValueError(\"backend 必須是 'ollama' 或 'lmstudio'\")\n",
    "        \n",
    "        print(f\"已初始化 {backend.upper()} 後端，使用模型：{self.model}\")\n",
    "\n",
    "    def chat(self, user_message):\n",
    "        \"\"\"發送訊息並獲得回應\"\"\"\n",
    "\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "        if self.backend == \"lmstudio\":\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=self.messages\n",
    "            )\n",
    "            ai_message = response.choices[0].message.content\n",
    "\n",
    "        else:  # ollama\n",
    "            data = {\n",
    "                \"model\": self.model,\n",
    "                \"messages\": self.messages,\n",
    "                \"stream\": False\n",
    "            }\n",
    "            response = requests.post(self.url, json=data)\n",
    "            result = response.json()\n",
    "            ai_message = result[\"message\"][\"content\"]\n",
    "\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": ai_message})\n",
    "        return ai_message\n",
    "    \n",
    "    def set_system_prompt(self, system_prompt):\n",
    "        \"\"\"設定系統提示詞\"\"\"\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"清除對話歷史\"\"\"\n",
    "        self.messages = []\n",
    "        print(\"對話歷史已清除！\")\n",
    "    \n",
    "    def get_backend_info(self):\n",
    "        \"\"\"取得後端資訊\"\"\"\n",
    "        return {\n",
    "            \"backend\": self.backend,\n",
    "            \"model\": self.model,\n",
    "            \"message_count\": len(self.messages)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: 使用 LM Studio 後端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立使用 LM Studio 的聊天機器人\n",
    "lm_bot = UniversalChatBot(backend=\"lmstudio\")\n",
    "\n",
    "print(\"\\n=== LM Studio 測試 ===\")\n",
    "response = lm_bot.chat(\"你好！請用一句話介紹你自己。\")\n",
    "print(f\"AI：{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: 使用 Ollama 後端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立使用 Ollama 的聊天機器人\n",
    "# 如果沒有運行 Ollama，這個會失敗\n",
    "try:\n",
    "    ollama_bot = UniversalChatBot(backend=\"ollama\")\n",
    "    print(\"\\n=== Ollama 測試 ===\")\n",
    "    response = ollama_bot.chat(\"你好！請用一句話介紹你自己。\")\n",
    "    print(f\"AI：{response}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ollama 連接失敗：{e}\")\n",
    "    print(\"請確認 Ollama 是否正在運行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: 比較不同後端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_backends(question):\n",
    "    \"\"\"\n",
    "    比較不同後端對同一問題的回答\n",
    "    \"\"\"\n",
    "    print(f\"問題：{question}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # LM Studio\n",
    "    try:\n",
    "        lm = UniversalChatBot(backend=\"lmstudio\")\n",
    "        lm_response = lm.chat(question)\n",
    "        print(f\"\\n[LM Studio]\")\n",
    "        print(lm_response[:300] + \"...\" if len(lm_response) > 300 else lm_response)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[LM Studio] 錯誤：{e}\")\n",
    "    \n",
    "    # Ollama\n",
    "    try:\n",
    "        ollama = UniversalChatBot(backend=\"ollama\")\n",
    "        ollama_response = ollama.chat(question)\n",
    "        print(f\"\\n[Ollama]\")\n",
    "        print(ollama_response[:300] + \"...\" if len(ollama_response) > 300 else ollama_response)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[Ollama] 錯誤：{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較兩個後端\n",
    "compare_backends(\"什麼是遞迴？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: 工廠函數設計模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chatbot(backend=\"auto\", model=None):\n",
    "    \"\"\"\n",
    "    工廠函數：自動選擇可用的後端\n",
    "    \n",
    "    參數：\n",
    "        backend: \"auto\", \"lmstudio\", 或 \"ollama\"\n",
    "        model: 模型名稱\n",
    "    \n",
    "    回傳：\n",
    "        UniversalChatBot 實例\n",
    "    \"\"\"\n",
    "    if backend != \"auto\":\n",
    "        return UniversalChatBot(backend=backend, model=model)\n",
    "    \n",
    "    # 自動檢測可用的後端\n",
    "    print(\"正在檢測可用的後端...\")\n",
    "    \n",
    "    # 先嘗試 LM Studio\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:1234/v1/models\", timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            print(\"找到 LM Studio！\")\n",
    "            return UniversalChatBot(backend=\"lmstudio\", model=model)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 再嘗試 Ollama\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            print(\"找到 Ollama！\")\n",
    "            return UniversalChatBot(backend=\"ollama\", model=model)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    raise RuntimeError(\"找不到可用的 AI 後端！請啟動 LM Studio 或 Ollama。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自動選擇後端\n",
    "try:\n",
    "    auto_bot = create_chatbot(backend=\"auto\")\n",
    "    print(f\"\\n使用的後端：{auto_bot.get_backend_info()}\")\n",
    "    print(f\"\\n回答：{auto_bot.chat('1+1等於多少？')}\")\n",
    "except Exception as e:\n",
    "    print(f\"錯誤：{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 架構設計說明\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────┐\n",
    "│         UniversalChatBot            │\n",
    "├─────────────────────────────────────┤\n",
    "│  - backend: str                     │\n",
    "│  - messages: list                   │\n",
    "│  - model: str                       │\n",
    "├─────────────────────────────────────┤\n",
    "│  + chat(message) -> str             │\n",
    "│  + set_system_prompt(prompt)        │\n",
    "│  + clear_history()                  │\n",
    "└─────────────────────────────────────┘\n",
    "              │\n",
    "    ┌─────────┴─────────┐\n",
    "    ▼                   ▼\n",
    "┌─────────┐       ┌──────────┐\n",
    "│ Ollama  │       │ LM Studio │\n",
    "│  API    │       │   API    │\n",
    "└─────────┘       └──────────┘\n",
    "```\n",
    "\n",
    "這種設計讓使用者不需要關心底層使用哪個 API！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立你自己的聊天機器人\n",
    "my_bot = create_chatbot(backend=\"auto\")\n",
    "\n",
    "# 設定角色\n",
    "my_bot.set_system_prompt(\"你是一位友善的助手，用繁體中文回答。\")\n",
    "\n",
    "# 開始對話\n",
    "print(my_bot.chat(\"你好！\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重點回顧\n",
    "\n",
    "1. **統一介面**：使用同一個類別處理不同後端\n",
    "2. **工廠模式**：使用 `create_chatbot()` 自動選擇後端\n",
    "3. **錯誤處理**：優雅地處理連接失敗\n",
    "4. **可擴展性**：容易加入新的後端支援\n",
    "\n",
    "## 下一步\n",
    "\n",
    "接下來我們將學習 RAG（檢索增強生成）技術！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
