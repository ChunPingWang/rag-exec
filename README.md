# ä½¿ç”¨ Ollama èˆ‡ LM Studio é‹è¡Œæœ¬åœ° AI æ¨¡å‹æ•™å­¸

> é©åˆé«˜ä¸­ç¨‹åº¦å­¸ç¿’è€…çš„æœ¬åœ° AI æ¨¡å‹å®Œæ•´æ•™å­¸ï¼Œæ¶µè“‹åŸºç¤åˆ°é€²éšæ‡‰ç”¨ã€‚

---

## ğŸ““ åˆ†æ”¯èªªæ˜ (Branch Information)

| åˆ†æ”¯ | èªªæ˜ | é©ç”¨å°è±¡ |
|------|------|----------|
| **`main`** (ç›®å‰åˆ†æ”¯) | Python è…³æœ¬æ ¼å¼ (.py)ï¼Œå¯ç›´æ¥åŸ·è¡Œ | é–‹ç™¼è€…ã€é€²éšä½¿ç”¨è€… |
| `jupyter-notebook` | Jupyter Notebook æ ¼å¼ï¼Œäº’å‹•å¼å­¸ç¿’é«”é©— | å­¸ç”Ÿã€åˆå­¸è€…ã€æ•™å­¸ç”¨é€” |

### ğŸ’» main åˆ†æ”¯ç‰¹è‰²

æ­¤åˆ†æ”¯åŒ…å«æ‰€æœ‰ç¯„ä¾‹çš„ **Python è…³æœ¬ (.py)** æ ¼å¼ï¼š

- **ç›´æ¥åŸ·è¡Œ**ï¼š`python example_01_basic_chat.py`
- **é©åˆé–‹ç™¼**ï¼šå¯æ•´åˆåˆ°å…¶ä»–å°ˆæ¡ˆä¸­
- **å‘½ä»¤åˆ—æ“ä½œ**ï¼šé©åˆç†Ÿæ‚‰çµ‚ç«¯æ©Ÿçš„ä½¿ç”¨è€…

### ğŸ“ Python æª”æ¡ˆåˆ—è¡¨

```
example_01_basic_chat.py        - åŸºæœ¬å°è©±
example_02_multi_turn_chat.py   - å¤šè¼ªå°è©±ï¼ˆæœ‰è¨˜æ†¶ï¼‰
example_03_streaming.py         - ä¸²æµè¼¸å‡º
example_04_system_prompt.py     - ç³»çµ±æç¤ºè©èˆ‡è§’è‰²æ‰®æ¼”
example_05_code_assistant.py    - ç¨‹å¼ç¢¼åŠ©æ‰‹
example_06_lmstudio_basic.py    - LM Studio åŸºæœ¬å°è©±
example_07_lmstudio_openai.py   - ä½¿ç”¨ OpenAI SDK
example_08_lmstudio_multi_turn.py - LM Studio å¤šè¼ªå°è©±
example_09_lmstudio_streaming.py  - LM Studio ä¸²æµè¼¸å‡º
example_10_list_models.py       - åˆ—å‡ºå¯ç”¨æ¨¡å‹
example_11_universal_chatbot.py - é€šç”¨èŠå¤©ç¨‹å¼
example_12_simple_rag.py        - ç°¡æ˜“ RAG ç³»çµ±
example_13_vector_rag.py        - å‘é‡æœå°‹ RAG
example_14_document_qa.py       - æ–‡ä»¶å•ç­”ç³»çµ±
example_15_prepare_dataset.py   - æº–å‚™ Fine-Tuning è³‡æ–™
example_16_ollama_modelfile.py  - Ollama è‡ªè¨‚æ¨¡å‹
example_17_data_augmentation.py - è³‡æ–™å¢å¼·
example_18_evaluate_finetune.py - è©•ä¼°æ¨¡å‹æ•ˆæœ
example_19_evaluation_system.py - å®Œæ•´è©•ä¼°ç³»çµ±
```

### ğŸ“ æƒ³è¦äº’å‹•å¼å­¸ç¿’ï¼Ÿ

åˆ‡æ›åˆ° `jupyter-notebook` åˆ†æ”¯ç²å¾— Jupyter Notebook ç‰ˆæœ¬ï¼š
```bash
git checkout jupyter-notebook
```

---

## ç›®éŒ„

### åŸºç¤æ¦‚å¿µ
- [ä»€éº¼æ˜¯å¤§å‹èªè¨€æ¨¡å‹ (LLM)ï¼Ÿ](#ä»€éº¼æ˜¯å¤§å‹èªè¨€æ¨¡å‹-llm)
- [ä»€éº¼æ˜¯ Ollamaï¼Ÿ](#ä»€éº¼æ˜¯-ollama)
- [ä»€éº¼æ˜¯ LM Studioï¼Ÿ](#ä»€éº¼æ˜¯-lm-studio)
- [ç’°å¢ƒæº–å‚™](#ç’°å¢ƒæº–å‚™)

### Ollama ç¯„ä¾‹ç¨‹å¼ç¢¼
- [ç¯„ä¾‹ 1ï¼šåŸºæœ¬å°è©±](#ç¯„ä¾‹-1åŸºæœ¬å°è©±)
- [ç¯„ä¾‹ 2ï¼šå¤šè¼ªå°è©±ï¼ˆæœ‰è¨˜æ†¶çš„èŠå¤©ï¼‰](#ç¯„ä¾‹-2å¤šè¼ªå°è©±æœ‰è¨˜æ†¶çš„èŠå¤©)
- [ç¯„ä¾‹ 3ï¼šä¸²æµè¼¸å‡ºï¼ˆå³æ™‚é¡¯ç¤ºï¼‰](#ç¯„ä¾‹-3ä¸²æµè¼¸å‡ºå³æ™‚é¡¯ç¤º)
- [ç¯„ä¾‹ 4ï¼šè¨­å®šç³»çµ±æç¤ºè©ï¼ˆè§’è‰²æ‰®æ¼”ï¼‰](#ç¯„ä¾‹-4è¨­å®šç³»çµ±æç¤ºè©è§’è‰²æ‰®æ¼”)
- [ç¯„ä¾‹ 5ï¼šç¨‹å¼ç¢¼åŠ©æ‰‹](#ç¯„ä¾‹-5ç¨‹å¼ç¢¼åŠ©æ‰‹)

### LM Studio ç¯„ä¾‹ç¨‹å¼ç¢¼
- [ç¯„ä¾‹ 6ï¼šLM Studio åŸºæœ¬å°è©±](#ç¯„ä¾‹-6lm-studio-åŸºæœ¬å°è©±ä½¿ç”¨-requests)
- [ç¯„ä¾‹ 7ï¼šLM Studio ä½¿ç”¨ OpenAI å¥—ä»¶](#ç¯„ä¾‹-7lm-studio-ä½¿ç”¨-openai-å¥—ä»¶)
- [ç¯„ä¾‹ 8ï¼šLM Studio å¤šè¼ªå°è©±](#ç¯„ä¾‹-8lm-studio-å¤šè¼ªå°è©±)
- [ç¯„ä¾‹ 9ï¼šLM Studio ä¸²æµè¼¸å‡º](#ç¯„ä¾‹-9lm-studio-ä¸²æµè¼¸å‡º)
- [ç¯„ä¾‹ 10ï¼šåˆ—å‡ºå¯ç”¨æ¨¡å‹](#ç¯„ä¾‹-10åˆ—å‡º-lm-studio-å¯ç”¨æ¨¡å‹)
- [ç¯„ä¾‹ 11ï¼šé€šç”¨èŠå¤©ç¨‹å¼](#ç¯„ä¾‹-11é€šç”¨èŠå¤©ç¨‹å¼æ”¯æ´-ollama-å’Œ-lm-studio)

### é‡è¦æ¦‚å¿µ
- [API æ˜¯ä»€éº¼ï¼Ÿ](#api-æ˜¯ä»€éº¼)
- [JSON æ˜¯ä»€éº¼ï¼Ÿ](#json-æ˜¯ä»€éº¼)
- [HTTP è«‹æ±‚æ˜¯ä»€éº¼ï¼Ÿ](#http-è«‹æ±‚æ˜¯ä»€éº¼)
- [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)

### é€²éšä¸»é¡Œï¼šRAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰
- [ä»€éº¼æ˜¯ RAGï¼Ÿ](#ä»€éº¼æ˜¯-rag)
- [ç¯„ä¾‹ 12ï¼šç°¡æ˜“ RAG ç³»çµ±](#ç¯„ä¾‹-12ç°¡æ˜“-rag-ç³»çµ±)
- [ç¯„ä¾‹ 13ï¼šå‘é‡æœå°‹ RAG](#ç¯„ä¾‹-13ä½¿ç”¨å‘é‡æœå°‹çš„-rag)
- [ç¯„ä¾‹ 14ï¼šæ–‡ä»¶å•ç­”ç³»çµ±](#ç¯„ä¾‹-14æ–‡ä»¶å•ç­”ç³»çµ±)

### é€²éšä¸»é¡Œï¼šFine-Tuningï¼ˆå¾®èª¿ï¼‰
- [ä»€éº¼æ˜¯ Fine-Tuningï¼Ÿ](#ä»€éº¼æ˜¯-fine-tuning)
- [ç¯„ä¾‹ 15ï¼šæº–å‚™è¨“ç·´è³‡æ–™é›†](#ç¯„ä¾‹-15æº–å‚™-fine-tuning-è³‡æ–™é›†)
- [ç¯„ä¾‹ 16ï¼šä½¿ç”¨ Ollama å»ºç«‹è‡ªè¨‚æ¨¡å‹](#ç¯„ä¾‹-16ä½¿ç”¨-ollama-é€²è¡Œ-fine-tuning)
- [ç¯„ä¾‹ 17ï¼šè³‡æ–™å¢å¼·](#ç¯„ä¾‹-17fine-tuning-è³‡æ–™å¢å¼·)
- [ç¯„ä¾‹ 18ï¼šè©•ä¼°æ¨¡å‹æ•ˆæœ](#ç¯„ä¾‹-18è©•ä¼°-fine-tuning-æ•ˆæœ)

### Fine-Tuning æˆæ•ˆè©•ä¼°
- [ç‚ºä»€éº¼è¦è©•ä¼°ï¼Ÿ](#ç‚ºä»€éº¼è¦è©•ä¼°)
- [è©•ä¼°æ–¹æ³•ä¸€ï¼šå®šé‡æŒ‡æ¨™](#è©•ä¼°æ–¹æ³•ä¸€å®šé‡æŒ‡æ¨™ç”¨æ•¸å­—è¡¡é‡)
- [è©•ä¼°æ–¹æ³•äºŒï¼šå®šæ€§è©•ä¼°](#è©•ä¼°æ–¹æ³•äºŒå®šæ€§è©•ä¼°äººå·¥åˆ¤æ–·)
- [è©•ä¼°æ–¹æ³•ä¸‰ï¼šA/B æ¸¬è©¦](#è©•ä¼°æ–¹æ³•ä¸‰ab-æ¸¬è©¦)
- [ç¯„ä¾‹ 19ï¼šå®Œæ•´è©•ä¼°ç³»çµ±](#ç¯„ä¾‹-19å®Œæ•´çš„æ¨¡å‹è©•ä¼°ç³»çµ±)
- [ç›£æ§è¨“ç·´éç¨‹](#ç›£æ§è¨“ç·´éç¨‹)
- [è©•ä¼°æª¢æŸ¥æ¸…å–®](#è©•ä¼°æª¢æŸ¥æ¸…å–®)

### é™„éŒ„
- [å»¶ä¼¸å­¸ç¿’](#å»¶ä¼¸å­¸ç¿’)
- [æˆæ¬Š](#æˆæ¬Š)

---

## ä»€éº¼æ˜¯å¤§å‹èªè¨€æ¨¡å‹ (LLM)ï¼Ÿ

å¤§å‹èªè¨€æ¨¡å‹å°±åƒä¸€å€‹è®€éå¤§é‡æ›¸ç±çš„ã€Œè¶…ç´šå­¸ç”Ÿã€ã€‚å®ƒé€éå­¸ç¿’ç¶²è·¯ä¸Šçš„æ–‡ç« ã€æ›¸ç±ã€ç¨‹å¼ç¢¼ç­‰è³‡æ–™ï¼Œå­¸æœƒäº†å¦‚ä½•ç†è§£å’Œç”Ÿæˆäººé¡èªè¨€ã€‚

**ç°¡å–®æ¯”å–»ï¼š**
- æƒ³åƒä½ è®€äº† 1000 æœ¬æ›¸å¾Œï¼Œèƒ½å¤ å›ç­”å„ç¨®å•é¡Œã€å¯«æ–‡ç« ã€ç”šè‡³å¹«äººè§£æ±ºå•é¡Œ
- LLM å°±æ˜¯è®€äº†ã€Œå¹¾ä¹æ•´å€‹ç¶²è·¯ã€çš„è³‡æ–™å¾Œï¼Œå­¸æœƒé€™äº›èƒ½åŠ›çš„ç¨‹å¼

## ä»€éº¼æ˜¯ Ollamaï¼Ÿ

Ollama æ˜¯ä¸€å€‹è®“ä½ åœ¨è‡ªå·±é›»è…¦ä¸Šé‹è¡Œ AI æ¨¡å‹çš„å·¥å…·ã€‚å°±åƒä½ åœ¨é›»è…¦ä¸Šå®‰è£éŠæˆ²ä¸€æ¨£ï¼ŒOllama è®“ä½ ã€Œå®‰è£ã€å’Œã€Œé‹è¡Œã€AI æ¨¡å‹ã€‚

**å„ªé»ï¼š**
- å…è²»ä½¿ç”¨
- è³‡æ–™ä¸æœƒä¸Šå‚³åˆ°ç¶²è·¯ï¼ˆéš±ç§å®‰å…¨ï¼‰
- ä¸éœ€è¦ç¶²è·¯ä¹Ÿèƒ½ä½¿ç”¨
- å‘½ä»¤åˆ—æ“ä½œï¼Œé©åˆé–‹ç™¼è€…

## ä»€éº¼æ˜¯ LM Studioï¼Ÿ

LM Studio æ˜¯å¦ä¸€å€‹æœ¬åœ°é‹è¡Œ AI æ¨¡å‹çš„å·¥å…·ï¼Œæä¾›åœ–å½¢åŒ–ä»‹é¢ï¼ˆGUIï¼‰ï¼Œæ›´é©åˆåˆå­¸è€…ä½¿ç”¨ã€‚

**å„ªé»ï¼š**
- åœ–å½¢åŒ–ä»‹é¢ï¼Œæ“ä½œç°¡å–®ç›´è¦º
- æ”¯æ´ OpenAI ç›¸å®¹ APIï¼ˆå¯ç›´æ¥ä½¿ç”¨ç¾æœ‰çš„ OpenAI ç¨‹å¼ç¢¼ï¼‰
- å¯ä»¥è¼•é¬†åˆ‡æ›ä¸åŒæ¨¡å‹
- å…§å»ºæ¨¡å‹ä¸‹è¼‰ç®¡ç†å™¨

**Ollama vs LM Studio æ¯”è¼ƒï¼š**

| ç‰¹é» | Ollama | LM Studio |
|------|--------|-----------|
| ä»‹é¢ | å‘½ä»¤åˆ— | åœ–å½¢åŒ– |
| API æ ¼å¼ | Ollama å°ˆç”¨ | OpenAI ç›¸å®¹ |
| é è¨­åŸ è™Ÿ | 11434 | 1234 |
| é©åˆå°è±¡ | é–‹ç™¼è€… | åˆå­¸è€…/ä¸€èˆ¬ä½¿ç”¨è€… |

---

## ç’°å¢ƒæº–å‚™

### æ–¹æ³•ä¸€ï¼šå®‰è£ Ollama

1. å‰å¾€ [Ollama å®˜ç¶²](https://ollama.com) ä¸‹è¼‰ä¸¦å®‰è£
2. é–‹å•Ÿçµ‚ç«¯æ©Ÿï¼Œä¸‹è¼‰æ¨¡å‹ï¼š
```bash
ollama pull gpt-oss:120b
```

### æ–¹æ³•äºŒï¼šå®‰è£ LM Studio

1. å‰å¾€ [LM Studio å®˜ç¶²](https://lmstudio.ai) ä¸‹è¼‰ä¸¦å®‰è£
2. é–‹å•Ÿ LM Studioï¼Œåœ¨æœå°‹æ¬„æœå°‹ä¸¦ä¸‹è¼‰æƒ³è¦çš„æ¨¡å‹
3. é»æ“Šå·¦å´ã€ŒLocal Serverã€åœ–ç¤ºï¼Œå•Ÿå‹•æœ¬åœ°ä¼ºæœå™¨

### å®‰è£ Python å¥—ä»¶

```bash
pip install requests openai
```

---

## Ollama Python ç¨‹å¼ç¢¼ç¯„ä¾‹

### ç¯„ä¾‹ 1ï¼šåŸºæœ¬å°è©±

```python
"""
ç¯„ä¾‹ 1ï¼šèˆ‡ AI é€²è¡Œç°¡å–®å°è©±
é€™æ˜¯æœ€åŸºæœ¬çš„ä½¿ç”¨æ–¹å¼ï¼Œå°±åƒè·Ÿ AI èŠå¤©ä¸€æ¨£
"""

import requests
import json

def chat_with_ai(prompt):
    """
    ç™¼é€è¨Šæ¯çµ¦ AI ä¸¦ç²å¾—å›æ‡‰

    åƒæ•¸ï¼š
        prompt: ä½ æƒ³å• AI çš„å•é¡Œï¼ˆå­—ä¸²ï¼‰

    å›å‚³ï¼š
        AI çš„å›æ‡‰ï¼ˆå­—ä¸²ï¼‰
    """

    # Ollama çš„ API ç¶²å€ï¼ˆåœ¨ä½ çš„é›»è…¦ä¸Šé‹è¡Œï¼‰
    url = "http://localhost:11434/api/generate"

    # æº–å‚™è¦ç™¼é€çš„è³‡æ–™
    data = {
        "model": "gpt-oss:120b",  # ä½¿ç”¨çš„æ¨¡å‹åç¨±
        "prompt": prompt,          # ä½ çš„å•é¡Œ
        "stream": False            # ä¸ä½¿ç”¨ä¸²æµï¼ˆä¸€æ¬¡è¿”å›å®Œæ•´å›æ‡‰ï¼‰
    }

    # ç™¼é€è«‹æ±‚çµ¦ Ollama
    response = requests.post(url, json=data)

    # è§£æå›æ‡‰
    result = response.json()

    return result["response"]


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    # å• AI ä¸€å€‹å•é¡Œ
    question = "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿè«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ã€‚"

    print(f"å•é¡Œï¼š{question}")
    print("-" * 50)

    answer = chat_with_ai(question)
    print(f"AI å›ç­”ï¼š{answer}")
```

---

### ç¯„ä¾‹ 2ï¼šå¤šè¼ªå°è©±ï¼ˆæœ‰è¨˜æ†¶çš„èŠå¤©ï¼‰

```python
"""
ç¯„ä¾‹ 2ï¼šå¤šè¼ªå°è©±
AI æœƒè¨˜ä½ä¹‹å‰çš„å°è©±å…§å®¹ï¼Œå°±åƒçœŸæ­£çš„èŠå¤©ä¸€æ¨£
"""

import requests
import json

class ChatBot:
    """
    èŠå¤©æ©Ÿå™¨äººé¡åˆ¥
    å¯ä»¥é€²è¡Œå¤šè¼ªå°è©±ï¼ŒAI æœƒè¨˜ä½å°è©±æ­·å²
    """

    def __init__(self):
        """åˆå§‹åŒ–èŠå¤©æ©Ÿå™¨äºº"""
        self.url = "http://localhost:11434/api/chat"
        self.model = "gpt-oss:120b"
        self.messages = []  # å„²å­˜å°è©±æ­·å²

    def chat(self, user_message):
        """
        ç™¼é€è¨Šæ¯ä¸¦ç²å¾—å›æ‡‰

        åƒæ•¸ï¼š
            user_message: ä½¿ç”¨è€…çš„è¨Šæ¯

        å›å‚³ï¼š
            AI çš„å›æ‡‰
        """

        # å°‡ä½¿ç”¨è€…è¨Šæ¯åŠ å…¥æ­·å²
        self.messages.append({
            "role": "user",
            "content": user_message
        })

        # æº–å‚™è«‹æ±‚è³‡æ–™
        data = {
            "model": self.model,
            "messages": self.messages,
            "stream": False
        }

        # ç™¼é€è«‹æ±‚
        response = requests.post(self.url, json=data)
        result = response.json()

        # å–å¾— AI å›æ‡‰
        ai_message = result["message"]["content"]

        # å°‡ AI å›æ‡‰åŠ å…¥æ­·å²ï¼ˆé€™æ¨£ AI å°±èƒ½è¨˜ä½ï¼‰
        self.messages.append({
            "role": "assistant",
            "content": ai_message
        })

        return ai_message

    def clear_history(self):
        """æ¸…é™¤å°è©±æ­·å²ï¼Œé–‹å§‹æ–°å°è©±"""
        self.messages = []
        print("å°è©±æ­·å²å·²æ¸…é™¤ï¼")


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    bot = ChatBot()

    print("=== å¤šè¼ªå°è©±ç¤ºç¯„ ===")
    print("è¼¸å…¥ 'quit' çµæŸå°è©±")
    print("è¼¸å…¥ 'clear' æ¸…é™¤å°è©±æ­·å²")
    print("-" * 50)

    while True:
        user_input = input("\nä½ ï¼š")

        if user_input.lower() == "quit":
            print("å†è¦‹ï¼")
            break
        elif user_input.lower() == "clear":
            bot.clear_history()
            continue

        response = bot.chat(user_input)
        print(f"\nAIï¼š{response}")
```

---

### ç¯„ä¾‹ 3ï¼šä¸²æµè¼¸å‡ºï¼ˆå³æ™‚é¡¯ç¤ºï¼‰

```python
"""
ç¯„ä¾‹ 3ï¼šä¸²æµè¼¸å‡º
åƒ ChatGPT ä¸€æ¨£ï¼Œä¸€å€‹å­—ä¸€å€‹å­—åœ°é¡¯ç¤ºå›æ‡‰
"""

import requests
import json

def stream_chat(prompt):
    """
    ä½¿ç”¨ä¸²æµæ–¹å¼ç²å¾— AI å›æ‡‰
    å›æ‡‰æœƒä¸€å€‹å­—ä¸€å€‹å­—åœ°é¡¯ç¤ºå‡ºä¾†
    """

    url = "http://localhost:11434/api/generate"

    data = {
        "model": "gpt-oss:120b",
        "prompt": prompt,
        "stream": True  # å•Ÿç”¨ä¸²æµæ¨¡å¼
    }

    # ä½¿ç”¨ä¸²æµæ–¹å¼ç™¼é€è«‹æ±‚
    response = requests.post(url, json=data, stream=True)

    print("AIï¼š", end="", flush=True)

    # é€è¡Œè®€å–å›æ‡‰
    for line in response.iter_lines():
        if line:
            # è§£ææ¯ä¸€è¡Œ JSON
            chunk = json.loads(line)

            # å°å‡ºé€™ä¸€å°æ®µæ–‡å­—ï¼ˆä¸æ›è¡Œï¼‰
            print(chunk["response"], end="", flush=True)

            # å¦‚æœå®Œæˆäº†ï¼Œå°±è·³å‡ºè¿´åœˆ
            if chunk.get("done", False):
                break

    print()  # æœ€å¾Œæ›è¡Œ


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    question = "è«‹å¯«ä¸€é¦–é—œæ–¼ç¨‹å¼è¨­è¨ˆçš„çŸ­è©©ã€‚"
    print(f"å•é¡Œï¼š{question}")
    print("-" * 50)
    stream_chat(question)
```

---

### ç¯„ä¾‹ 4ï¼šè¨­å®šç³»çµ±æç¤ºè©ï¼ˆè§’è‰²æ‰®æ¼”ï¼‰

```python
"""
ç¯„ä¾‹ 4ï¼šè¨­å®šç³»çµ±æç¤ºè©
å¯ä»¥è®“ AI æ‰®æ¼”ç‰¹å®šè§’è‰²ï¼Œä¾‹å¦‚ï¼šè€å¸«ã€ç¿»è­¯å®˜ã€ç¨‹å¼å°ˆå®¶ç­‰
"""

import requests
import json

def chat_with_role(system_prompt, user_message):
    """
    ä½¿ç”¨ç‰¹å®šè§’è‰²èˆ‡ AI å°è©±

    åƒæ•¸ï¼š
        system_prompt: ç³»çµ±æç¤ºè©ï¼Œå®šç¾© AI çš„è§’è‰²å’Œè¡Œç‚º
        user_message: ä½¿ç”¨è€…çš„è¨Šæ¯
    """

    url = "http://localhost:11434/api/chat"

    data = {
        "model": "gpt-oss:120b",
        "messages": [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": user_message
            }
        ],
        "stream": False
    }

    response = requests.post(url, json=data)
    result = response.json()

    return result["message"]["content"]


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    # ç¯„ä¾‹ï¼šè®“ AI æ‰®æ¼”é«˜ä¸­æ•¸å­¸è€å¸«
    system = """ä½ æ˜¯ä¸€ä½è¦ªåˆ‡çš„é«˜ä¸­æ•¸å­¸è€å¸«ã€‚
    - ç”¨ç°¡å–®æ˜“æ‡‚çš„æ–¹å¼è§£é‡‹æ•¸å­¸æ¦‚å¿µ
    - å¤šèˆ‰ç”Ÿæ´»ä¸­çš„ä¾‹å­
    - é¼“å‹µå­¸ç”Ÿï¼Œä¿æŒæ­£å‘æ…‹åº¦
    - ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”"""

    question = "ä»€éº¼æ˜¯å¾®ç©åˆ†ï¼Ÿç‚ºä»€éº¼è¦å­¸å®ƒï¼Ÿ"

    print("è§’è‰²ï¼šé«˜ä¸­æ•¸å­¸è€å¸«")
    print(f"å•é¡Œï¼š{question}")
    print("-" * 50)

    answer = chat_with_role(system, question)
    print(f"è€å¸«ï¼š{answer}")
```

---

### ç¯„ä¾‹ 5ï¼šç¨‹å¼ç¢¼åŠ©æ‰‹

```python
"""
ç¯„ä¾‹ 5ï¼šç¨‹å¼ç¢¼åŠ©æ‰‹
è®“ AI å¹«ä½ è§£é‡‹ç¨‹å¼ç¢¼ã€æ‰¾éŒ¯èª¤ã€æˆ–å¯«ç¨‹å¼
"""

import requests
import json

def code_assistant(code, question):
    """
    ç¨‹å¼ç¢¼åŠ©æ‰‹ï¼šåˆ†æç¨‹å¼ç¢¼ä¸¦å›ç­”å•é¡Œ

    åƒæ•¸ï¼š
        code: è¦åˆ†æçš„ç¨‹å¼ç¢¼
        question: é—œæ–¼ç¨‹å¼ç¢¼çš„å•é¡Œ
    """

    url = "http://localhost:11434/api/chat"

    system_prompt = """ä½ æ˜¯ä¸€ä½ç¨‹å¼è¨­è¨ˆå°ˆå®¶å’Œæ•™å¸«ã€‚
    - ç”¨æ¸…æ¥šæ˜“æ‡‚çš„æ–¹å¼è§£é‡‹ç¨‹å¼ç¢¼
    - å¦‚æœç™¼ç¾éŒ¯èª¤ï¼ŒæŒ‡å‡ºéŒ¯èª¤ä¸¦æä¾›ä¿®æ­£å»ºè­°
    - ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
    - è§£é‡‹æ™‚è¦è€ƒæ…®åˆ°å­¸ç¿’è€…å¯èƒ½æ˜¯åˆå­¸è€…"""

    user_prompt = f"""è«‹åˆ†æä»¥ä¸‹ç¨‹å¼ç¢¼ä¸¦å›ç­”å•é¡Œã€‚

ç¨‹å¼ç¢¼ï¼š
```
{code}
```

å•é¡Œï¼š{question}"""

    data = {
        "model": "gpt-oss:120b",
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "stream": False
    }

    response = requests.post(url, json=data)
    result = response.json()

    return result["message"]["content"]


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    # è¦åˆ†æçš„ç¨‹å¼ç¢¼
    my_code = '''
def calculate_average(numbers):
    total = 0
    for num in numbers:
        total += num
    return total / len(numbers)

result = calculate_average([])
print(result)
'''

    question = "é€™æ®µç¨‹å¼ç¢¼æœ‰ä»€éº¼å•é¡Œï¼Ÿå¦‚ä½•ä¿®æ­£ï¼Ÿ"

    print("=== ç¨‹å¼ç¢¼åŠ©æ‰‹ ===")
    print(f"å•é¡Œï¼š{question}")
    print("-" * 50)

    answer = code_assistant(my_code, question)
    print(f"AI åˆ†æï¼š\n{answer}")
```

---

## LM Studio Python ç¨‹å¼ç¢¼ç¯„ä¾‹

LM Studio ä½¿ç”¨ OpenAI ç›¸å®¹çš„ APIï¼Œæ‰€ä»¥å¯ä»¥ä½¿ç”¨ `openai` å¥—ä»¶æˆ– `requests` ç›´æ¥å‘¼å«ã€‚

### ç¯„ä¾‹ 6ï¼šLM Studio åŸºæœ¬å°è©±ï¼ˆä½¿ç”¨ requestsï¼‰

```python
"""
ç¯„ä¾‹ 6ï¼šä½¿ç”¨ LM Studio é€²è¡ŒåŸºæœ¬å°è©±
LM Studio ä½¿ç”¨ OpenAI ç›¸å®¹çš„ API æ ¼å¼
"""

import requests

def chat_with_lmstudio(message):
    """
    ç™¼é€è¨Šæ¯çµ¦ LM Studio ä¸¦ç²å¾—å›æ‡‰

    åƒæ•¸ï¼š
        message: ä½ æƒ³å• AI çš„å•é¡Œ

    å›å‚³ï¼š
        AI çš„å›æ‡‰
    """

    # LM Studio çš„ API ç¶²å€ï¼ˆé è¨­åŸ è™Ÿ 1234ï¼‰
    url = "http://localhost:1234/v1/chat/completions"

    # æº–å‚™è¦ç™¼é€çš„è³‡æ–™ï¼ˆOpenAI æ ¼å¼ï¼‰
    data = {
        "model": "gpt-oss-120b",  # æ¨¡å‹åç¨±ï¼ˆåœ¨ LM Studio ä¸­è¼‰å…¥çš„æ¨¡å‹ï¼‰
        "messages": [
            {
                "role": "user",
                "content": message
            }
        ]
    }

    # ç™¼é€è«‹æ±‚
    response = requests.post(url, json=data)
    result = response.json()

    # å¾å›æ‡‰ä¸­å–å¾— AI çš„è¨Šæ¯
    return result["choices"][0]["message"]["content"]


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    question = "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿè«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ã€‚"

    print(f"å•é¡Œï¼š{question}")
    print("-" * 50)

    answer = chat_with_lmstudio(question)
    print(f"AI å›ç­”ï¼š{answer}")
```

---

### ç¯„ä¾‹ 7ï¼šLM Studio ä½¿ç”¨ OpenAI å¥—ä»¶

```python
"""
ç¯„ä¾‹ 7ï¼šä½¿ç”¨ OpenAI å¥—ä»¶é€£æ¥ LM Studio
é€™ç¨®æ–¹å¼çš„å¥½è™•æ˜¯ï¼šå¦‚æœä½ ä¹‹å‰ç”¨é OpenAI APIï¼Œç¨‹å¼ç¢¼å¹¾ä¹ä¸ç”¨æ”¹ï¼
"""

from openai import OpenAI

def chat_with_openai_sdk(message):
    """
    ä½¿ç”¨ OpenAI SDK é€£æ¥ LM Studio

    åƒæ•¸ï¼š
        message: ä½ æƒ³å• AI çš„å•é¡Œ

    å›å‚³ï¼š
        AI çš„å›æ‡‰
    """

    # å»ºç«‹ OpenAI å®¢æˆ¶ç«¯ï¼ŒæŒ‡å‘ LM Studio
    client = OpenAI(
        base_url="http://localhost:1234/v1",  # LM Studio çš„ç¶²å€
        api_key="not-needed"                   # LM Studio ä¸éœ€è¦ API é‡‘é‘°
    )

    # ç™¼é€èŠå¤©è«‹æ±‚
    response = client.chat.completions.create(
        model="gpt-oss-120b",  # æ¨¡å‹åç¨±
        messages=[
            {"role": "user", "content": message}
        ]
    )

    # å–å¾—å›æ‡‰
    return response.choices[0].message.content


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    question = "è«‹è§£é‡‹ä»€éº¼æ˜¯è¿´åœˆï¼Œä¸¦çµ¦æˆ‘ä¸€å€‹ Python ç¯„ä¾‹ã€‚"

    print(f"å•é¡Œï¼š{question}")
    print("-" * 50)

    answer = chat_with_openai_sdk(question)
    print(f"AI å›ç­”ï¼š{answer}")
```

---

### ç¯„ä¾‹ 8ï¼šLM Studio å¤šè¼ªå°è©±

```python
"""
ç¯„ä¾‹ 8ï¼šLM Studio å¤šè¼ªå°è©±
ä½¿ç”¨ OpenAI SDK å¯¦ç¾æœ‰è¨˜æ†¶çš„å°è©±
"""

from openai import OpenAI

class LMStudioChatBot:
    """
    LM Studio èŠå¤©æ©Ÿå™¨äºº
    æ”¯æ´å¤šè¼ªå°è©±ï¼ŒAI æœƒè¨˜ä½å°è©±æ­·å²
    """

    def __init__(self, model_name="gpt-oss-120b"):
        """åˆå§‹åŒ–èŠå¤©æ©Ÿå™¨äºº"""
        self.client = OpenAI(
            base_url="http://localhost:1234/v1",
            api_key="not-needed"
        )
        self.model = model_name
        self.messages = []

    def set_system_prompt(self, system_prompt):
        """
        è¨­å®šç³»çµ±æç¤ºè©ï¼ˆAI çš„è§’è‰²ï¼‰

        åƒæ•¸ï¼š
            system_prompt: æè¿° AI è§’è‰²çš„æ–‡å­—
        """
        self.messages = [{"role": "system", "content": system_prompt}]

    def chat(self, user_message):
        """
        ç™¼é€è¨Šæ¯ä¸¦ç²å¾—å›æ‡‰

        åƒæ•¸ï¼š
            user_message: ä½¿ç”¨è€…çš„è¨Šæ¯

        å›å‚³ï¼š
            AI çš„å›æ‡‰
        """
        # åŠ å…¥ä½¿ç”¨è€…è¨Šæ¯
        self.messages.append({"role": "user", "content": user_message})

        # ç™¼é€è«‹æ±‚
        response = self.client.chat.completions.create(
            model=self.model,
            messages=self.messages
        )

        # å–å¾— AI å›æ‡‰
        ai_message = response.choices[0].message.content

        # åŠ å…¥ AI å›æ‡‰åˆ°æ­·å²
        self.messages.append({"role": "assistant", "content": ai_message})

        return ai_message

    def clear_history(self):
        """æ¸…é™¤å°è©±æ­·å²"""
        self.messages = []
        print("å°è©±æ­·å²å·²æ¸…é™¤ï¼")


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    bot = LMStudioChatBot()

    # è¨­å®š AI è§’è‰²ç‚ºè‹±æ–‡è€å¸«
    bot.set_system_prompt("""ä½ æ˜¯ä¸€ä½å‹å–„çš„è‹±æ–‡è€å¸«ã€‚
    - ç”¨ç¹é«”ä¸­æ–‡è§£é‡‹è‹±æ–‡æ–‡æ³•å’Œå–®å­—
    - æä¾›å¯¦ç”¨çš„ä¾‹å¥
    - é¼“å‹µå­¸ç”Ÿå¤šç·´ç¿’""")

    print("=== è‹±æ–‡è€å¸«èŠå¤©å®¤ ===")
    print("è¼¸å…¥ 'quit' çµæŸå°è©±")
    print("-" * 50)

    while True:
        user_input = input("\nä½ ï¼š")

        if user_input.lower() == "quit":
            print("å†è¦‹ï¼Keep learning!")
            break

        response = bot.chat(user_input)
        print(f"\nè€å¸«ï¼š{response}")
```

---

### ç¯„ä¾‹ 9ï¼šLM Studio ä¸²æµè¼¸å‡º

```python
"""
ç¯„ä¾‹ 9ï¼šLM Studio ä¸²æµè¼¸å‡º
å³æ™‚é¡¯ç¤º AI çš„å›æ‡‰ï¼Œåƒ ChatGPT ä¸€æ¨£ä¸€å€‹å­—ä¸€å€‹å­—å‡ºç¾
"""

from openai import OpenAI

def stream_chat_lmstudio(message):
    """
    ä½¿ç”¨ä¸²æµæ–¹å¼ç²å¾— LM Studio å›æ‡‰
    """

    client = OpenAI(
        base_url="http://localhost:1234/v1",
        api_key="not-needed"
    )

    # ç™¼é€ä¸²æµè«‹æ±‚
    stream = client.chat.completions.create(
        model="gpt-oss-120b",
        messages=[{"role": "user", "content": message}],
        stream=True  # å•Ÿç”¨ä¸²æµæ¨¡å¼
    )

    print("AIï¼š", end="", flush=True)

    # é€æ­¥æ¥æ”¶ä¸¦é¡¯ç¤ºå›æ‡‰
    for chunk in stream:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="", flush=True)

    print()  # æœ€å¾Œæ›è¡Œ


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    question = "è«‹ç”¨ä¸‰å¥è©±ä»‹ç´¹å°ç£ã€‚"
    print(f"å•é¡Œï¼š{question}")
    print("-" * 50)
    stream_chat_lmstudio(question)
```

---

### ç¯„ä¾‹ 10ï¼šåˆ—å‡º LM Studio å¯ç”¨æ¨¡å‹

```python
"""
ç¯„ä¾‹ 10ï¼šæŸ¥çœ‹ LM Studio ä¸­å¯ç”¨çš„æ¨¡å‹
é€™å€‹ç¯„ä¾‹å±•ç¤ºå¦‚ä½•å–å¾— LM Studio ç›®å‰è¼‰å…¥çš„æ¨¡å‹æ¸…å–®
"""

import requests

def list_models():
    """
    å–å¾— LM Studio ä¸­å¯ç”¨çš„æ¨¡å‹æ¸…å–®
    """

    url = "http://localhost:1234/v1/models"

    response = requests.get(url)
    result = response.json()

    print("=== LM Studio å¯ç”¨æ¨¡å‹ ===")
    print("-" * 40)

    for model in result["data"]:
        print(f"â€¢ {model['id']}")

    return result["data"]


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    models = list_models()
    print(f"\nå…± {len(models)} å€‹æ¨¡å‹å¯ç”¨")
```

---

### ç¯„ä¾‹ 11ï¼šé€šç”¨èŠå¤©ç¨‹å¼ï¼ˆæ”¯æ´ Ollama å’Œ LM Studioï¼‰

```python
"""
ç¯„ä¾‹ 11ï¼šé€šç”¨èŠå¤©ç¨‹å¼
å¯ä»¥åœ¨ Ollama å’Œ LM Studio ä¹‹é–“åˆ‡æ›
"""

import requests
from openai import OpenAI

class UniversalChatBot:
    """
    é€šç”¨èŠå¤©æ©Ÿå™¨äºº
    æ”¯æ´ Ollama å’Œ LM Studio å…©ç¨®å¾Œç«¯
    """

    def __init__(self, backend="lmstudio", model=None):
        """
        åˆå§‹åŒ–èŠå¤©æ©Ÿå™¨äºº

        åƒæ•¸ï¼š
            backend: "ollama" æˆ– "lmstudio"
            model: æ¨¡å‹åç¨±ï¼ˆå¯é¸ï¼‰
        """
        self.backend = backend
        self.messages = []

        if backend == "lmstudio":
            self.client = OpenAI(
                base_url="http://localhost:1234/v1",
                api_key="not-needed"
            )
            self.model = model or "gpt-oss-120b"
        elif backend == "ollama":
            self.url = "http://localhost:11434/api/chat"
            self.model = model or "gpt-oss:120b"
        else:
            raise ValueError("backend å¿…é ˆæ˜¯ 'ollama' æˆ– 'lmstudio'")

    def chat(self, user_message):
        """ç™¼é€è¨Šæ¯ä¸¦ç²å¾—å›æ‡‰"""

        self.messages.append({"role": "user", "content": user_message})

        if self.backend == "lmstudio":
            response = self.client.chat.completions.create(
                model=self.model,
                messages=self.messages
            )
            ai_message = response.choices[0].message.content

        else:  # ollama
            data = {
                "model": self.model,
                "messages": self.messages,
                "stream": False
            }
            response = requests.post(self.url, json=data)
            result = response.json()
            ai_message = result["message"]["content"]

        self.messages.append({"role": "assistant", "content": ai_message})
        return ai_message


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    # é¸æ“‡å¾Œç«¯ï¼šæ”¹æˆ "ollama" å¯åˆ‡æ›åˆ° Ollama
    bot = UniversalChatBot(backend="lmstudio")

    print(f"=== ä½¿ç”¨ {bot.backend.upper()} å¾Œç«¯ ===")
    print("è¼¸å…¥ 'quit' çµæŸå°è©±")
    print("-" * 50)

    while True:
        user_input = input("\nä½ ï¼š")

        if user_input.lower() == "quit":
            print("å†è¦‹ï¼")
            break

        response = bot.chat(user_input)
        print(f"\nAIï¼š{response}")
```

---

## é‡è¦æ¦‚å¿µè§£é‡‹

### API æ˜¯ä»€éº¼ï¼Ÿ

APIï¼ˆApplication Programming Interfaceï¼‰å°±åƒé¤å»³çš„èœå–®å’Œé»é¤ç³»çµ±ï¼š

1. **ä½ ï¼ˆç¨‹å¼ï¼‰** = é¡§å®¢
2. **API** = æœå‹™ç”Ÿ + èœå–®
3. **Ollama** = å»šæˆ¿

æµç¨‹ï¼š
```
ä½ çš„ç¨‹å¼ â†’ é€é API ç™¼é€è«‹æ±‚ â†’ Ollama è™•ç† â†’ é€é API è¿”å›çµæœ â†’ ä½ çš„ç¨‹å¼æ”¶åˆ°å›æ‡‰
```

### JSON æ˜¯ä»€éº¼ï¼Ÿ

JSON æ˜¯ä¸€ç¨®è³‡æ–™æ ¼å¼ï¼Œå°±åƒå¡«å¯«è¡¨æ ¼ä¸€æ¨£ï¼Œæœ‰å›ºå®šçš„æ ¼å¼ï¼š

```python
# é€™æ˜¯ä¸€å€‹ JSON æ ¼å¼çš„è³‡æ–™
{
    "name": "å°æ˜",      # éµ: å€¼
    "age": 16,           # å¯ä»¥æ˜¯æ•¸å­—
    "hobbies": ["ç¨‹å¼", "éŸ³æ¨‚"]  # å¯ä»¥æ˜¯æ¸…å–®
}
```

### HTTP è«‹æ±‚æ˜¯ä»€éº¼ï¼Ÿ

å°±åƒå¯„ä¿¡ä¸€æ¨£ï¼š
- **POST è«‹æ±‚**ï¼šå¯„å‡ºä¸€å°ä¿¡ï¼Œè£¡é¢æœ‰å…§å®¹ï¼ˆä½ çš„å•é¡Œï¼‰
- **å›æ‡‰**ï¼šæ”¶åˆ°å›ä¿¡ï¼ˆAI çš„ç­”æ¡ˆï¼‰

---

## å¸¸è¦‹å•é¡Œ

### Q1: ç‚ºä»€éº¼ AI å›æ‡‰å¾ˆæ…¢ï¼Ÿ

**åŸå› ï¼š** 120B æ¨¡å‹éå¸¸å¤§ï¼ˆ1200 å„„åƒæ•¸ï¼‰ï¼Œéœ€è¦å¤§é‡è¨ˆç®—ã€‚

**è§£æ±ºæ–¹æ³•ï¼š**
- ä½¿ç”¨è¼ƒå°çš„æ¨¡å‹ï¼ˆå¦‚ 7Bã€13Bï¼‰
- ç¢ºä¿é›»è…¦æœ‰è¶³å¤ çš„è¨˜æ†¶é«”å’Œ GPU

### Q2: å‡ºç¾ã€Œé€£ç·šéŒ¯èª¤ã€æ€éº¼è¾¦ï¼Ÿ

**Ollama è§£æ±ºæ­¥é©Ÿï¼š**
1. ç¢ºèª Ollama æ­£åœ¨é‹è¡Œï¼šåœ¨çµ‚ç«¯æ©Ÿè¼¸å…¥ `ollama list`
2. ç¢ºèªæ¨¡å‹å·²ä¸‹è¼‰ï¼š`ollama pull gpt-oss:120b`
3. é‡å•Ÿ Ollama æœå‹™

**LM Studio è§£æ±ºæ­¥é©Ÿï¼š**
1. ç¢ºèª LM Studio å·²é–‹å•Ÿ
2. ç¢ºèªå·²å•Ÿå‹• Local Serverï¼ˆå·¦å´é¢æ¿ï¼‰
3. ç¢ºèªæœ‰è¼‰å…¥æ¨¡å‹ï¼ˆæ¨¡å‹åç¨±æœƒé¡¯ç¤ºåœ¨ä¸Šæ–¹ï¼‰

### Q3: å¦‚ä½•è®“ AI å›ç­”æ›´æº–ç¢ºï¼Ÿ

**æŠ€å·§ï¼š**
1. å•é¡Œè¦å…·é«”æ˜ç¢º
2. æä¾›è¶³å¤ çš„èƒŒæ™¯è³‡è¨Š
3. ä½¿ç”¨ç³»çµ±æç¤ºè©è¨­å®š AI çš„è§’è‰²

---

## é€²éšä¸»é¡Œï¼šRAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰

### ä»€éº¼æ˜¯ RAGï¼Ÿ

RAGï¼ˆRetrieval-Augmented Generationï¼Œæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰æ˜¯ä¸€ç¨®è®“ AIã€ŒæŸ¥è³‡æ–™å¾Œå†å›ç­”ã€çš„æŠ€è¡“ã€‚

**ç”Ÿæ´»æ¯”å–»ï¼š**
- **æ²’æœ‰ RAG çš„ AI**ï¼šåƒä¸€å€‹åªé è¨˜æ†¶å›ç­”å•é¡Œçš„å­¸ç”Ÿï¼Œå¯èƒ½æœƒè¨˜éŒ¯æˆ–ä¸çŸ¥é“æœ€æ–°è³‡è¨Š
- **æœ‰ RAG çš„ AI**ï¼šåƒä¸€å€‹å¯ä»¥ç¿»èª²æœ¬ã€æŸ¥ç­†è¨˜å¾Œå†å›ç­”çš„å­¸ç”Ÿï¼Œç­”æ¡ˆæ›´æº–ç¢º

**RAG çš„é‹ä½œæµç¨‹ï¼š**
```
ä½¿ç”¨è€…å•å•é¡Œ â†’ æœå°‹ç›¸é—œæ–‡ä»¶ â†’ æŠŠæ–‡ä»¶å…§å®¹çµ¦ AI åƒè€ƒ â†’ AI æ ¹æ“šæ–‡ä»¶å›ç­”
```

**ç‚ºä»€éº¼éœ€è¦ RAGï¼Ÿ**
1. AI çš„çŸ¥è­˜æœ‰æˆªæ­¢æ—¥æœŸï¼Œç„¡æ³•çŸ¥é“æœ€æ–°è³‡è¨Š
2. AI å¯èƒ½æœƒã€Œå¹»è¦ºã€ï¼ˆç·¨é€ ä¸å­˜åœ¨çš„è³‡è¨Šï¼‰
3. ä¼æ¥­éœ€è¦ AI å›ç­”å…¬å¸å…§éƒ¨çš„å°ˆæœ‰çŸ¥è­˜

---

### ç¯„ä¾‹ 12ï¼šç°¡æ˜“ RAG ç³»çµ±

```python
"""
ç¯„ä¾‹ 12ï¼šç°¡æ˜“ RAG ç³»çµ±
é€™å€‹ç¯„ä¾‹å±•ç¤º RAG çš„åŸºæœ¬æ¦‚å¿µï¼šå…ˆæœå°‹æ–‡ä»¶ï¼Œå†è®“ AI æ ¹æ“šæ–‡ä»¶å›ç­”
"""

from openai import OpenAI

# æ¨¡æ“¬çš„çŸ¥è­˜åº«ï¼ˆå¯¦éš›æ‡‰ç”¨ä¸­å¯èƒ½æ˜¯è³‡æ–™åº«æˆ–æ–‡ä»¶ç³»çµ±ï¼‰
KNOWLEDGE_BASE = {
    "python": """
    Python æ˜¯ä¸€ç¨®é«˜éšç¨‹å¼èªè¨€ï¼Œç”± Guido van Rossum æ–¼ 1991 å¹´å‰µå»ºã€‚
    Python çš„ç‰¹é»ï¼š
    - èªæ³•ç°¡æ½”æ˜“è®€
    - æ”¯æ´å¤šç¨®ç¨‹å¼è¨­è¨ˆç¯„å¼
    - æ“æœ‰è±å¯Œçš„ç¬¬ä¸‰æ–¹å¥—ä»¶
    - å»£æ³›ç”¨æ–¼ç¶²é é–‹ç™¼ã€è³‡æ–™ç§‘å­¸ã€äººå·¥æ™ºæ…§ç­‰é ˜åŸŸ
    """,
    "javascript": """
    JavaScript æ˜¯ä¸€ç¨®è…³æœ¬èªè¨€ï¼Œä¸»è¦ç”¨æ–¼ç¶²é é–‹ç™¼ã€‚
    JavaScript çš„ç‰¹é»ï¼š
    - å¯åœ¨ç€è¦½å™¨ä¸­ç›´æ¥åŸ·è¡Œ
    - æ”¯æ´äº‹ä»¶é©…å‹•ç¨‹å¼è¨­è¨ˆ
    - å¯ç”¨æ–¼å‰ç«¯å’Œå¾Œç«¯ï¼ˆNode.jsï¼‰é–‹ç™¼
    - æ˜¯ç¶²é äº’å‹•åŠŸèƒ½çš„æ ¸å¿ƒæŠ€è¡“
    """,
    "æ©Ÿå™¨å­¸ç¿’": """
    æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹åˆ†æ”¯ï¼Œè®“é›»è…¦å¾è³‡æ–™ä¸­å­¸ç¿’è¦å¾‹ã€‚
    æ©Ÿå™¨å­¸ç¿’çš„é¡å‹ï¼š
    - ç›£ç£å¼å­¸ç¿’ï¼šä½¿ç”¨æœ‰æ¨™ç±¤çš„è³‡æ–™è¨“ç·´
    - éç›£ç£å¼å­¸ç¿’ï¼šå¾ç„¡æ¨™ç±¤è³‡æ–™ä¸­ç™¼ç¾æ¨¡å¼
    - å¼·åŒ–å­¸ç¿’ï¼šé€éçå‹µæ©Ÿåˆ¶å­¸ç¿’æœ€ä½³ç­–ç•¥
    å¸¸è¦‹æ‡‰ç”¨ï¼šåœ–åƒè¾¨è­˜ã€èªéŸ³è¾¨è­˜ã€æ¨è–¦ç³»çµ±ç­‰
    """
}


def simple_search(query):
    """
    ç°¡å–®çš„é—œéµå­—æœå°‹
    åœ¨å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œé€™è£¡æœƒä½¿ç”¨å‘é‡æœå°‹æˆ–å…¨æ–‡æœå°‹å¼•æ“

    åƒæ•¸ï¼š
        query: æœå°‹é—œéµå­—

    å›å‚³ï¼š
        æ‰¾åˆ°çš„ç›¸é—œæ–‡ä»¶å…§å®¹
    """
    results = []
    query_lower = query.lower()

    for keyword, content in KNOWLEDGE_BASE.items():
        if keyword.lower() in query_lower or query_lower in keyword.lower():
            results.append(content)

    return results


def rag_chat(question):
    """
    RAG èŠå¤©å‡½æ•¸
    å…ˆæœå°‹ç›¸é—œæ–‡ä»¶ï¼Œå†è®“ AI æ ¹æ“šæ–‡ä»¶å›ç­”

    åƒæ•¸ï¼š
        question: ä½¿ç”¨è€…çš„å•é¡Œ

    å›å‚³ï¼š
        AI çš„å›ç­”
    """

    # æ­¥é©Ÿ 1ï¼šæœå°‹ç›¸é—œæ–‡ä»¶
    retrieved_docs = simple_search(question)

    # æ­¥é©Ÿ 2ï¼šå»ºç«‹æç¤ºè©
    if retrieved_docs:
        context = "\n\n".join(retrieved_docs)
        prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹åƒè€ƒè³‡æ–™å›ç­”å•é¡Œã€‚å¦‚æœåƒè€ƒè³‡æ–™ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹èªªæ˜ä½ ä¸ç¢ºå®šã€‚

åƒè€ƒè³‡æ–™ï¼š
{context}

å•é¡Œï¼š{question}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼š"""
    else:
        prompt = f"""å•é¡Œï¼š{question}

æ³¨æ„ï¼šæˆ‘æ‰¾ä¸åˆ°ç›¸é—œçš„åƒè€ƒè³‡æ–™ï¼Œè«‹æ ¹æ“šä½ çš„çŸ¥è­˜å›ç­”ï¼Œä½†è¦èªªæ˜é€™æ˜¯ä½ çš„ä¸€èˆ¬çŸ¥è­˜ï¼Œä¸æ˜¯ä¾†è‡ªç‰¹å®šæ–‡ä»¶ã€‚

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼š"""

    # æ­¥é©Ÿ 3ï¼šå‘¼å« AI
    client = OpenAI(
        base_url="http://localhost:1234/v1",
        api_key="not-needed"
    )

    response = client.chat.completions.create(
        model="gpt-oss-120b",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    print("=== ç°¡æ˜“ RAG ç³»çµ± ===")
    print("å¯ä»¥å•é—œæ–¼ Pythonã€JavaScriptã€æ©Ÿå™¨å­¸ç¿’çš„å•é¡Œ")
    print("è¼¸å…¥ 'quit' çµæŸ")
    print("-" * 50)

    while True:
        question = input("\nä½ çš„å•é¡Œï¼š")

        if question.lower() == "quit":
            break

        answer = rag_chat(question)
        print(f"\nAI å›ç­”ï¼š{answer}")
```

---

### ç¯„ä¾‹ 13ï¼šä½¿ç”¨å‘é‡æœå°‹çš„ RAG

```python
"""
ç¯„ä¾‹ 13ï¼šä½¿ç”¨å‘é‡æœå°‹çš„ RAG ç³»çµ±
é€™å€‹ç¯„ä¾‹ä½¿ç”¨å‘é‡åµŒå…¥ï¼ˆEmbeddingï¼‰ä¾†æœå°‹æœ€ç›¸é—œçš„æ–‡ä»¶

éœ€è¦å®‰è£ï¼špip install numpy
"""

import numpy as np
from openai import OpenAI

# åˆå§‹åŒ–å®¢æˆ¶ç«¯
client = OpenAI(
    base_url="http://localhost:1234/v1",
    api_key="not-needed"
)

# çŸ¥è­˜åº«æ–‡ä»¶
DOCUMENTS = [
    "Python æ˜¯ä¸€ç¨®ç°¡å–®æ˜“å­¸çš„ç¨‹å¼èªè¨€ï¼Œé©åˆåˆå­¸è€…å…¥é–€ã€‚",
    "JavaScript æ˜¯ç¶²é é–‹ç™¼çš„æ ¸å¿ƒèªè¨€ï¼Œå¯ä»¥è®“ç¶²é ç”¢ç”Ÿäº’å‹•æ•ˆæœã€‚",
    "æ©Ÿå™¨å­¸ç¿’è®“é›»è…¦èƒ½å¾è³‡æ–™ä¸­å­¸ç¿’ï¼Œä¸éœ€è¦æ˜ç¢ºçš„ç¨‹å¼æŒ‡ä»¤ã€‚",
    "æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹åˆ†æ”¯ï¼Œä½¿ç”¨ç¥ç¶“ç¶²è·¯ä¾†è™•ç†è¤‡é›œå•é¡Œã€‚",
    "è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰è®“é›»è…¦èƒ½ç†è§£å’Œç”Ÿæˆäººé¡èªè¨€ã€‚",
    "RAG æŠ€è¡“çµåˆäº†è³‡è¨Šæª¢ç´¢å’Œæ–‡å­—ç”Ÿæˆï¼Œæé«˜ AI å›ç­”çš„æº–ç¢ºæ€§ã€‚",
]

# å„²å­˜æ–‡ä»¶çš„å‘é‡è¡¨ç¤º
document_embeddings = []


def get_embedding(text):
    """
    å–å¾—æ–‡å­—çš„å‘é‡è¡¨ç¤ºï¼ˆEmbeddingï¼‰

    å‘é‡åµŒå…¥æ˜¯ä»€éº¼ï¼Ÿ
    - æŠŠæ–‡å­—è½‰æ›æˆä¸€ä¸²æ•¸å­—ï¼ˆå‘é‡ï¼‰
    - æ„æ€ç›¸è¿‘çš„æ–‡å­—ï¼Œå‘é‡ä¹Ÿæœƒç›¸è¿‘
    - é€™æ¨£é›»è…¦å°±èƒ½ã€Œç†è§£ã€æ–‡å­—çš„æ„ç¾©
    """
    response = client.embeddings.create(
        model="text-embedding-nomic-embed-text-v1.5",  # åµŒå…¥æ¨¡å‹
        input=text
    )
    return response.data[0].embedding


def cosine_similarity(vec1, vec2):
    """
    è¨ˆç®—å…©å€‹å‘é‡çš„é¤˜å¼¦ç›¸ä¼¼åº¦

    é¤˜å¼¦ç›¸ä¼¼åº¦æ˜¯ä»€éº¼ï¼Ÿ
    - è¡¡é‡å…©å€‹å‘é‡æ–¹å‘çš„ç›¸ä¼¼ç¨‹åº¦
    - å€¼ä»‹æ–¼ -1 åˆ° 1 ä¹‹é–“
    - 1 è¡¨ç¤ºå®Œå…¨ç›¸åŒï¼Œ0 è¡¨ç¤ºç„¡é—œï¼Œ-1 è¡¨ç¤ºå®Œå…¨ç›¸å
    """
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))


def initialize_knowledge_base():
    """
    åˆå§‹åŒ–çŸ¥è­˜åº«ï¼šç‚ºæ‰€æœ‰æ–‡ä»¶è¨ˆç®—å‘é‡
    """
    global document_embeddings
    print("æ­£åœ¨åˆå§‹åŒ–çŸ¥è­˜åº«...")

    for doc in DOCUMENTS:
        embedding = get_embedding(doc)
        document_embeddings.append(embedding)

    print(f"å·²è¼‰å…¥ {len(DOCUMENTS)} ä»½æ–‡ä»¶")


def vector_search(query, top_k=2):
    """
    å‘é‡æœå°‹ï¼šæ‰¾å‡ºèˆ‡å•é¡Œæœ€ç›¸é—œçš„æ–‡ä»¶

    åƒæ•¸ï¼š
        query: ä½¿ç”¨è€…çš„å•é¡Œ
        top_k: è¦è¿”å›çš„æ–‡ä»¶æ•¸é‡

    å›å‚³ï¼š
        æœ€ç›¸é—œçš„æ–‡ä»¶åˆ—è¡¨
    """
    # å–å¾—å•é¡Œçš„å‘é‡
    query_embedding = get_embedding(query)

    # è¨ˆç®—èˆ‡æ¯ä»½æ–‡ä»¶çš„ç›¸ä¼¼åº¦
    similarities = []
    for i, doc_embedding in enumerate(document_embeddings):
        similarity = cosine_similarity(query_embedding, doc_embedding)
        similarities.append((similarity, DOCUMENTS[i]))

    # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–å‰ k å€‹
    similarities.sort(reverse=True)
    return [doc for _, doc in similarities[:top_k]]


def rag_with_vector_search(question):
    """
    ä½¿ç”¨å‘é‡æœå°‹çš„ RAG èŠå¤©
    """
    # æœå°‹ç›¸é—œæ–‡ä»¶
    relevant_docs = vector_search(question, top_k=2)

    # å»ºç«‹æç¤ºè©
    context = "\n".join([f"- {doc}" for doc in relevant_docs])

    prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹åƒè€ƒè³‡æ–™å›ç­”å•é¡Œï¼š

åƒè€ƒè³‡æ–™ï¼š
{context}

å•é¡Œï¼š{question}

è«‹ç”¨ç¹é«”ä¸­æ–‡ç°¡æ½”å›ç­”ï¼š"""

    # å‘¼å« AI
    response = client.chat.completions.create(
        model="gpt-oss-120b",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content, relevant_docs


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    # åˆå§‹åŒ–çŸ¥è­˜åº«
    initialize_knowledge_base()

    print("\n=== å‘é‡æœå°‹ RAG ç³»çµ± ===")
    print("è¼¸å…¥ 'quit' çµæŸ")
    print("-" * 50)

    while True:
        question = input("\nä½ çš„å•é¡Œï¼š")

        if question.lower() == "quit":
            break

        answer, sources = rag_with_vector_search(question)

        print(f"\næ‰¾åˆ°çš„ç›¸é—œæ–‡ä»¶ï¼š")
        for i, doc in enumerate(sources, 1):
            print(f"  {i}. {doc[:50]}...")

        print(f"\nAI å›ç­”ï¼š{answer}")
```

---

### ç¯„ä¾‹ 14ï¼šæ–‡ä»¶å•ç­”ç³»çµ±

```python
"""
ç¯„ä¾‹ 14ï¼šæ–‡ä»¶å•ç­”ç³»çµ±
è®€å–æ–‡å­—æª”æ¡ˆï¼Œè®“ AI å›ç­”é—œæ–¼æ–‡ä»¶å…§å®¹çš„å•é¡Œ
"""

import os
from openai import OpenAI


def read_document(file_path):
    """
    è®€å–æ–‡ä»¶å…§å®¹

    åƒæ•¸ï¼š
        file_path: æ–‡ä»¶è·¯å¾‘

    å›å‚³ï¼š
        æ–‡ä»¶å…§å®¹
    """
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()


def chunk_document(text, chunk_size=500, overlap=50):
    """
    å°‡é•·æ–‡ä»¶åˆ‡å‰²æˆå°æ®µè½

    ç‚ºä»€éº¼è¦åˆ‡å‰²ï¼Ÿ
    - AI æœ‰è¼¸å…¥é•·åº¦é™åˆ¶
    - å°æ®µè½æ›´å®¹æ˜“ç²¾ç¢ºæœå°‹
    - å¯ä»¥åªå‚³é€ç›¸é—œçš„éƒ¨åˆ†çµ¦ AI

    åƒæ•¸ï¼š
        text: æ–‡ä»¶å…§å®¹
        chunk_size: æ¯æ®µçš„å­—æ•¸
        overlap: æ®µè½ä¹‹é–“é‡ç–Šçš„å­—æ•¸ï¼ˆé¿å…è³‡è¨Šè¢«åˆ‡æ–·ï¼‰
    """
    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - overlap  # é‡ç–Šéƒ¨åˆ†

    return chunks


class DocumentQA:
    """
    æ–‡ä»¶å•ç­”ç³»çµ±é¡åˆ¥
    """

    def __init__(self):
        self.client = OpenAI(
            base_url="http://localhost:1234/v1",
            api_key="not-needed"
        )
        self.chunks = []

    def load_document(self, file_path):
        """è¼‰å…¥æ–‡ä»¶"""
        text = read_document(file_path)
        self.chunks = chunk_document(text)
        print(f"å·²è¼‰å…¥æ–‡ä»¶ï¼Œå…± {len(self.chunks)} å€‹æ®µè½")

    def load_text(self, text):
        """ç›´æ¥è¼‰å…¥æ–‡å­—"""
        self.chunks = chunk_document(text)
        print(f"å·²è¼‰å…¥æ–‡å­—ï¼Œå…± {len(self.chunks)} å€‹æ®µè½")

    def find_relevant_chunks(self, question, top_k=3):
        """
        æ‰¾å‡ºèˆ‡å•é¡Œç›¸é—œçš„æ®µè½ï¼ˆç°¡å–®çš„é—œéµå­—åŒ¹é…ï¼‰
        """
        scored_chunks = []

        # å°‡å•é¡Œæ‹†æˆé—œéµå­—
        keywords = question.lower().split()

        for chunk in self.chunks:
            score = 0
            chunk_lower = chunk.lower()

            # è¨ˆç®—æ¯å€‹é—œéµå­—å‡ºç¾çš„æ¬¡æ•¸
            for keyword in keywords:
                if keyword in chunk_lower:
                    score += chunk_lower.count(keyword)

            scored_chunks.append((score, chunk))

        # æ’åºä¸¦è¿”å›æœ€ç›¸é—œçš„æ®µè½
        scored_chunks.sort(reverse=True)
        return [chunk for score, chunk in scored_chunks[:top_k] if score > 0]

    def ask(self, question):
        """
        æå•ä¸¦ç²å¾—å›ç­”
        """
        # æ‰¾å‡ºç›¸é—œæ®µè½
        relevant_chunks = self.find_relevant_chunks(question)

        if not relevant_chunks:
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨æ–‡ä»¶ä¸­æ‰¾ä¸åˆ°ç›¸é—œè³‡è¨Šã€‚"

        # å»ºç«‹ä¸Šä¸‹æ–‡
        context = "\n---\n".join(relevant_chunks)

        prompt = f"""ä½ æ˜¯ä¸€å€‹æ–‡ä»¶å•ç­”åŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹æ–‡ä»¶å…§å®¹å›ç­”å•é¡Œã€‚
å¦‚æœæ–‡ä»¶ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹èª å¯¦èªªæ˜ã€‚

æ–‡ä»¶å…§å®¹ï¼š
{context}

å•é¡Œï¼š{question}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼š"""

        response = self.client.chat.completions.create(
            model="gpt-oss-120b",
            messages=[{"role": "user", "content": prompt}]
        )

        return response.choices[0].message.content


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    qa = DocumentQA()

    # ç¯„ä¾‹ï¼šè¼‰å…¥ä¸€æ®µèªªæ˜æ–‡å­—
    sample_text = """
    äººå·¥æ™ºæ…§ï¼ˆArtificial Intelligenceï¼Œç°¡ç¨± AIï¼‰æ˜¯é›»è…¦ç§‘å­¸çš„ä¸€å€‹åˆ†æ”¯ï¼Œ
    è‡´åŠ›æ–¼å‰µé€ èƒ½å¤ åŸ·è¡Œé€šå¸¸éœ€è¦äººé¡æ™ºæ…§çš„ä»»å‹™çš„æ©Ÿå™¨ã€‚é€™äº›ä»»å‹™åŒ…æ‹¬å­¸ç¿’ã€
    æ¨ç†ã€å•é¡Œè§£æ±ºã€æ„ŸçŸ¥å’Œèªè¨€ç†è§£ã€‚

    æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹å­é ˜åŸŸï¼Œå°ˆæ³¨æ–¼é–‹ç™¼èƒ½å¤ å¾è³‡æ–™ä¸­å­¸ç¿’çš„æ¼”ç®—æ³•ã€‚
    æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹åˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²è·¯ä¾†è™•ç†è¤‡é›œçš„è³‡æ–™æ¨¡å¼ã€‚

    è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰æ˜¯ AI çš„å¦ä¸€å€‹é‡è¦é ˜åŸŸï¼Œè®“é›»è…¦èƒ½å¤ ç†è§£ã€è§£é‡‹å’Œ
    ç”Ÿæˆäººé¡èªè¨€ã€‚ChatGPT å°±æ˜¯ä¸€å€‹è‘—åçš„ NLP æ‡‰ç”¨ã€‚

    AI çš„æ‡‰ç”¨éå¸¸å»£æ³›ï¼ŒåŒ…æ‹¬ï¼š
    - èªéŸ³åŠ©æ‰‹ï¼ˆå¦‚ Siriã€Alexaï¼‰
    - è‡ªå‹•é§•é§›æ±½è»Š
    - é†«ç™‚è¨ºæ–·è¼”åŠ©
    - æ¨è–¦ç³»çµ±ï¼ˆå¦‚ Netflixã€YouTubeï¼‰
    - éŠæˆ² AI
    """

    qa.load_text(sample_text)

    print("\n=== æ–‡ä»¶å•ç­”ç³»çµ± ===")
    print("è¼¸å…¥ 'quit' çµæŸ")
    print("-" * 50)

    while True:
        question = input("\nä½ çš„å•é¡Œï¼š")

        if question.lower() == "quit":
            break

        answer = qa.ask(question)
        print(f"\nå›ç­”ï¼š{answer}")
```

---

## é€²éšä¸»é¡Œï¼šFine-Tuningï¼ˆå¾®èª¿ï¼‰

### ä»€éº¼æ˜¯ Fine-Tuningï¼Ÿ

Fine-Tuningï¼ˆå¾®èª¿ï¼‰æ˜¯åœ¨å·²è¨“ç·´å¥½çš„æ¨¡å‹åŸºç¤ä¸Šï¼Œç”¨ç‰¹å®šè³‡æ–™é€²è¡Œé¡å¤–è¨“ç·´ï¼Œè®“æ¨¡å‹æ›´é©åˆç‰¹å®šä»»å‹™ã€‚

**ç”Ÿæ´»æ¯”å–»ï¼š**
- **åŸå§‹æ¨¡å‹**ï¼šåƒä¸€å€‹å—éé€šè­˜æ•™è‚²çš„å­¸ç”Ÿï¼Œä»€éº¼éƒ½çŸ¥é“ä¸€é»
- **Fine-Tuning**ï¼šåƒè®“é€™å€‹å­¸ç”Ÿå°ˆæ”»æŸå€‹é ˜åŸŸï¼ˆå¦‚æ³•å¾‹ã€é†«å­¸ï¼‰ï¼Œè®Šæˆå°ˆå®¶

**Fine-Tuning vs RAG æ¯”è¼ƒï¼š**

| ç‰¹é» | Fine-Tuning | RAG |
|------|-------------|-----|
| çŸ¥è­˜å„²å­˜ | å­˜åœ¨æ¨¡å‹åƒæ•¸ä¸­ | å­˜åœ¨å¤–éƒ¨è³‡æ–™åº« |
| æ›´æ–°çŸ¥è­˜ | éœ€è¦é‡æ–°è¨“ç·´ | åªéœ€æ›´æ–°è³‡æ–™åº« |
| è¨ˆç®—è³‡æº | éœ€è¦ GPU è¨“ç·´ | åªéœ€æ¨ç†è³‡æº |
| é©ç”¨å ´æ™¯ | æ”¹è®Šæ¨¡å‹è¡Œç‚ºé¢¨æ ¼ | éœ€è¦æœ€æ–°/å°ˆæœ‰çŸ¥è­˜ |
| æˆæœ¬ | è¼ƒé«˜ | è¼ƒä½ |

**ä»€éº¼æ™‚å€™ç”¨ Fine-Tuningï¼Ÿ**
- éœ€è¦æ¨¡å‹ç”¨ç‰¹å®šé¢¨æ ¼/èªæ°£å›ç­”
- éœ€è¦æ¨¡å‹åŸ·è¡Œç‰¹å®šæ ¼å¼çš„ä»»å‹™
- éœ€è¦æé«˜ç‰¹å®šé ˜åŸŸçš„æº–ç¢ºæ€§

**ä»€éº¼æ™‚å€™ç”¨ RAGï¼Ÿ**
- éœ€è¦æœ€æ–°è³‡è¨Š
- éœ€è¦å¼•ç”¨ä¾†æº
- çŸ¥è­˜æœƒé »ç¹æ›´æ–°

---

### ç¯„ä¾‹ 15ï¼šæº–å‚™ Fine-Tuning è³‡æ–™é›†

```python
"""
ç¯„ä¾‹ 15ï¼šæº–å‚™ Fine-Tuning è³‡æ–™é›†
é€™å€‹ç¯„ä¾‹å±•ç¤ºå¦‚ä½•æº–å‚™ç”¨æ–¼å¾®èª¿çš„è¨“ç·´è³‡æ–™

Fine-Tuning è³‡æ–™æ ¼å¼é€šå¸¸æ˜¯ã€Œå°è©±ã€å½¢å¼ï¼š
- è¼¸å…¥ï¼ˆå•é¡Œ/æŒ‡ä»¤ï¼‰
- è¼¸å‡ºï¼ˆæœŸæœ›çš„å›ç­”ï¼‰
"""

import json


def create_training_example(instruction, input_text, output):
    """
    å»ºç«‹ä¸€ç­†è¨“ç·´è³‡æ–™

    åƒæ•¸ï¼š
        instruction: ä»»å‹™æŒ‡ä»¤
        input_text: è¼¸å…¥å…§å®¹ï¼ˆå¯ç‚ºç©ºï¼‰
        output: æœŸæœ›çš„è¼¸å‡º

    å›å‚³ï¼š
        æ ¼å¼åŒ–çš„è¨“ç·´è³‡æ–™
    """
    if input_text:
        text = f"### æŒ‡ä»¤ï¼š\n{instruction}\n\n### è¼¸å…¥ï¼š\n{input_text}\n\n### å›ç­”ï¼š\n{output}"
    else:
        text = f"### æŒ‡ä»¤ï¼š\n{instruction}\n\n### å›ç­”ï¼š\n{output}"

    return {"text": text}


def create_chat_example(system, user, assistant):
    """
    å»ºç«‹å°è©±æ ¼å¼çš„è¨“ç·´è³‡æ–™ï¼ˆOpenAI æ ¼å¼ï¼‰

    åƒæ•¸ï¼š
        system: ç³»çµ±æç¤ºè©
        user: ä½¿ç”¨è€…è¨Šæ¯
        assistant: AI å›ç­”
    """
    return {
        "messages": [
            {"role": "system", "content": system},
            {"role": "user", "content": user},
            {"role": "assistant", "content": assistant}
        ]
    }


# ç¯„ä¾‹ï¼šå»ºç«‹å®¢æœæ©Ÿå™¨äººçš„è¨“ç·´è³‡æ–™
def create_customer_service_dataset():
    """
    å»ºç«‹å®¢æœæ©Ÿå™¨äººçš„è¨“ç·´è³‡æ–™é›†
    """

    system_prompt = "ä½ æ˜¯ä¸€å€‹å‹å–„çš„ç·šä¸Šå•†åº—å®¢æœåŠ©æ‰‹ï¼Œå°ˆé–€å›ç­”é—œæ–¼è¨‚å–®å’Œç”¢å“çš„å•é¡Œã€‚"

    training_data = [
        create_chat_example(
            system_prompt,
            "æˆ‘çš„è¨‚å–®ä»€éº¼æ™‚å€™æœƒåˆ°ï¼Ÿ",
            "æ‚¨å¥½ï¼ä¸€èˆ¬è¨‚å–®æœƒåœ¨ 3-5 å€‹å·¥ä½œå¤©å…§é€é”ã€‚å¦‚æœæ‚¨èƒ½æä¾›è¨‚å–®ç·¨è™Ÿï¼Œæˆ‘å¯ä»¥å¹«æ‚¨æŸ¥è©¢æ›´æº–ç¢ºçš„é…é€ç‹€æ…‹ã€‚è«‹å•æ‚¨çš„è¨‚å–®ç·¨è™Ÿæ˜¯å¤šå°‘å‘¢ï¼Ÿ"
        ),
        create_chat_example(
            system_prompt,
            "æˆ‘æƒ³é€€è²¨",
            "æ²’å•é¡Œï¼æˆ‘å€‘æä¾› 7 å¤©å…§ç„¡æ¢ä»¶é€€è²¨æœå‹™ã€‚è«‹ç¢ºä¿å•†å“ä¿æŒåŸåŒ…è£ä¸”æœªä½¿ç”¨ã€‚æ‚¨å¯ä»¥é€éä»¥ä¸‹æ­¥é©Ÿç”³è«‹é€€è²¨ï¼š\n1. ç™»å…¥æœƒå“¡ä¸­å¿ƒ\n2. é»é¸ã€Œæˆ‘çš„è¨‚å–®ã€\n3. é¸æ“‡è¦é€€è²¨çš„å•†å“\n4. å¡«å¯«é€€è²¨åŸå› \n\nè«‹å•éœ€è¦æˆ‘å”åŠ©æ‚¨å…¶ä»–äº‹é …å—ï¼Ÿ"
        ),
        create_chat_example(
            system_prompt,
            "é€™å€‹ç”¢å“æœ‰ä¿å›ºå—ï¼Ÿ",
            "æ˜¯çš„ï¼Œæˆ‘å€‘æ‰€æœ‰ç”¢å“éƒ½æä¾›åŸå» ä¿å›ºï¼ä¿å›ºæœŸé™ä¾ç”¢å“é¡å‹è€Œå®šï¼š\n- é›»å­ç”¢å“ï¼š1 å¹´ä¿å›º\n- å®¶é›»ç”¢å“ï¼š2 å¹´ä¿å›º\n- é…ä»¶é¡ï¼š6 å€‹æœˆä¿å›º\n\nè«‹å•æ‚¨æƒ³äº†è§£å“ªå€‹ç”¢å“çš„ä¿å›ºè³‡è¨Šå‘¢ï¼Ÿ"
        ),
        create_chat_example(
            system_prompt,
            "å¯ä»¥æ”¹å¯„é€åœ°å€å—ï¼Ÿ",
            "å¯ä»¥çš„ï¼å¦‚æœè¨‚å–®å°šæœªå‡ºè²¨ï¼Œæ‚¨å¯ä»¥ä¿®æ”¹å¯„é€åœ°å€ã€‚è«‹æä¾›ï¼š\n1. æ‚¨çš„è¨‚å–®ç·¨è™Ÿ\n2. æ–°çš„å¯„é€åœ°å€\n\næˆ‘æœƒç›¡å¿«ç‚ºæ‚¨è™•ç†ã€‚ä¸éå¦‚æœè¨‚å–®å·²ç¶“å‡ºè²¨ï¼Œå°±éœ€è¦è¯ç¹«ç‰©æµå…¬å¸é€²è¡Œè½‰å¯„å›‰ã€‚"
        ),
        create_chat_example(
            system_prompt,
            "ä»˜æ¬¾å¤±æ•—æ€éº¼è¾¦ï¼Ÿ",
            "åˆ¥æ“”å¿ƒï¼ä»˜æ¬¾å¤±æ•—å¯èƒ½æœ‰å¹¾å€‹åŸå› ï¼š\n1. ä¿¡ç”¨å¡é¡åº¦ä¸è¶³\n2. éŠ€è¡Œæ‹’çµ•äº¤æ˜“\n3. ç¶²è·¯é€£ç·šå•é¡Œ\n\nå»ºè­°æ‚¨å¯ä»¥ï¼š\n- ç¢ºèªä¿¡ç”¨å¡ç‹€æ…‹æ­£å¸¸\n- å˜—è©¦ä½¿ç”¨å…¶ä»–ä»˜æ¬¾æ–¹å¼\n- ç¨å¾Œé‡æ–°ä¸‹å–®\n\nå¦‚æœå•é¡ŒæŒçºŒç™¼ç”Ÿï¼Œè«‹æä¾›éŒ¯èª¤è¨Šæ¯ï¼Œæˆ‘å¯ä»¥é€²ä¸€æ­¥å”åŠ©æ‚¨ï¼"
        ),
    ]

    return training_data


def save_dataset(data, filename):
    """
    å„²å­˜è³‡æ–™é›†ç‚º JSONL æ ¼å¼
    ï¼ˆæ¯ä¸€è¡Œæ˜¯ä¸€å€‹ JSON ç‰©ä»¶ï¼‰
    """
    with open(filename, "w", encoding="utf-8") as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    print(f"å·²å„²å­˜ {len(data)} ç­†è³‡æ–™åˆ° {filename}")


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    # å»ºç«‹å®¢æœè³‡æ–™é›†
    dataset = create_customer_service_dataset()

    # é¡¯ç¤ºè³‡æ–™é›†å…§å®¹
    print("=== è¨“ç·´è³‡æ–™é›†é è¦½ ===\n")
    for i, item in enumerate(dataset[:2], 1):
        print(f"--- ç¯„ä¾‹ {i} ---")
        for msg in item["messages"]:
            print(f"{msg['role'].upper()}: {msg['content'][:100]}...")
        print()

    # å„²å­˜è³‡æ–™é›†
    save_dataset(dataset, "customer_service_training.jsonl")
```

---

### ç¯„ä¾‹ 16ï¼šä½¿ç”¨ Ollama é€²è¡Œ Fine-Tuning

```python
"""
ç¯„ä¾‹ 16ï¼šä½¿ç”¨ Ollama å»ºç«‹è‡ªè¨‚æ¨¡å‹
Ollama æ”¯æ´é€é Modelfile å»ºç«‹è‡ªè¨‚æ¨¡å‹

æ³¨æ„ï¼šé€™ä¸æ˜¯çœŸæ­£çš„ Fine-Tuningï¼Œè€Œæ˜¯é€éç³»çµ±æç¤ºè©ä¾†ã€Œå®šåˆ¶ã€æ¨¡å‹è¡Œç‚º
çœŸæ­£çš„ Fine-Tuning éœ€è¦ä½¿ç”¨å°ˆé–€çš„è¨“ç·´æ¡†æ¶ï¼ˆå¦‚ Hugging Faceã€Axolotl ç­‰ï¼‰
"""

import subprocess
import os


def create_modelfile(base_model, system_prompt, model_name):
    """
    å»ºç«‹ Ollama Modelfile

    åƒæ•¸ï¼š
        base_model: åŸºç¤æ¨¡å‹åç¨±
        system_prompt: ç³»çµ±æç¤ºè©
        model_name: æ–°æ¨¡å‹åç¨±
    """

    modelfile_content = f'''FROM {base_model}

SYSTEM """
{system_prompt}
"""

PARAMETER temperature 0.7
PARAMETER top_p 0.9
'''

    # å„²å­˜ Modelfile
    modelfile_path = f"Modelfile_{model_name}"
    with open(modelfile_path, "w", encoding="utf-8") as f:
        f.write(modelfile_content)

    print(f"å·²å»ºç«‹ Modelfile: {modelfile_path}")
    return modelfile_path


def create_ollama_model(modelfile_path, model_name):
    """
    ä½¿ç”¨ Ollama å»ºç«‹æ¨¡å‹

    åƒæ•¸ï¼š
        modelfile_path: Modelfile è·¯å¾‘
        model_name: æ–°æ¨¡å‹åç¨±
    """
    print(f"æ­£åœ¨å»ºç«‹æ¨¡å‹ {model_name}...")

    try:
        result = subprocess.run(
            ["ollama", "create", model_name, "-f", modelfile_path],
            capture_output=True,
            text=True
        )

        if result.returncode == 0:
            print(f"æ¨¡å‹ {model_name} å»ºç«‹æˆåŠŸï¼")
            print("ä½¿ç”¨æ–¹å¼ï¼šollama run " + model_name)
        else:
            print(f"å»ºç«‹å¤±æ•—ï¼š{result.stderr}")

    except FileNotFoundError:
        print("æ‰¾ä¸åˆ° ollama æŒ‡ä»¤ï¼Œè«‹ç¢ºèª Ollama å·²å®‰è£")


# ç¯„ä¾‹ï¼šå»ºç«‹ä¸€å€‹ç¨‹å¼æ•™å­¸åŠ©æ‰‹
if __name__ == "__main__":
    # å®šç¾©ç³»çµ±æç¤ºè©
    system_prompt = """ä½ æ˜¯ä¸€ä½è¦ªåˆ‡çš„ç¨‹å¼è¨­è¨ˆæ•™å¸«ï¼Œå°ˆé–€æ•™å°åˆå­¸è€…å­¸ç¿’ç¨‹å¼ã€‚

ä½ çš„ç‰¹é»ï¼š
- ä½¿ç”¨ç°¡å–®æ˜“æ‡‚çš„èªè¨€è§£é‡‹æ¦‚å¿µ
- æä¾›å¤§é‡çš„ç¨‹å¼ç¢¼ç¯„ä¾‹
- ç”¨ç”Ÿæ´»ä¸­çš„ä¾‹å­ä¾†æ¯”å–»æŠ½è±¡æ¦‚å¿µ
- é¼“å‹µå­¸ç”Ÿï¼Œä¿æŒæ­£å‘æ…‹åº¦
- å¦‚æœå­¸ç”ŸçŠ¯éŒ¯ï¼Œè€å¿ƒè§£é‡‹éŒ¯èª¤åŸå› 
- ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”

å›ç­”æ ¼å¼ï¼š
1. å…ˆç°¡å–®è§£é‡‹æ¦‚å¿µ
2. æä¾›ç¨‹å¼ç¢¼ç¯„ä¾‹
3. è§£é‡‹ç¨‹å¼ç¢¼çš„æ¯å€‹éƒ¨åˆ†
4. çµ¦äºˆç·´ç¿’å»ºè­°"""

    # å»ºç«‹ Modelfile
    modelfile_path = create_modelfile(
        base_model="gpt-oss:120b",
        system_prompt=system_prompt,
        model_name="programming-teacher"
    )

    print("\n" + "=" * 50)
    print("Modelfile å…§å®¹é è¦½ï¼š")
    print("=" * 50)
    with open(modelfile_path, "r", encoding="utf-8") as f:
        print(f.read())

    print("\nè¦å»ºç«‹æ¨¡å‹ï¼Œè«‹åŸ·è¡Œä»¥ä¸‹æŒ‡ä»¤ï¼š")
    print(f"  ollama create programming-teacher -f {modelfile_path}")
    print("\nå»ºç«‹å®Œæˆå¾Œï¼Œä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤åŸ·è¡Œï¼š")
    print("  ollama run programming-teacher")
```

---

### ç¯„ä¾‹ 17ï¼šFine-Tuning è³‡æ–™å¢å¼·

```python
"""
ç¯„ä¾‹ 17ï¼šFine-Tuning è³‡æ–™å¢å¼·
ä½¿ç”¨ AI ä¾†å¹«åŠ©ç”Ÿæˆæ›´å¤šè¨“ç·´è³‡æ–™

è³‡æ–™å¢å¼·æ˜¯ä»€éº¼ï¼Ÿ
- ç”¨å°‘é‡çš„ç¨®å­è³‡æ–™ï¼Œç”Ÿæˆæ›´å¤šé¡ä¼¼çš„è¨“ç·´è³‡æ–™
- å¯ä»¥å¢åŠ è¨“ç·´è³‡æ–™çš„å¤šæ¨£æ€§
- æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
"""

from openai import OpenAI
import json


client = OpenAI(
    base_url="http://localhost:1234/v1",
    api_key="not-needed"
)


def augment_qa_pair(question, answer, num_variations=3):
    """
    ç”Ÿæˆå•ç­”å°çš„è®Šé«”

    åƒæ•¸ï¼š
        question: åŸå§‹å•é¡Œ
        answer: åŸå§‹å›ç­”
        num_variations: è¦ç”Ÿæˆçš„è®Šé«”æ•¸é‡

    å›å‚³ï¼š
        å•ç­”å°è®Šé«”åˆ—è¡¨
    """

    prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹å•ç­”å°ï¼Œç”Ÿæˆ {num_variations} å€‹é¡ä¼¼ä½†ä¸åŒçš„å•ç­”è®Šé«”ã€‚
ä¿æŒå›ç­”çš„æ ¸å¿ƒè³‡è¨Šç›¸åŒï¼Œä½†æ”¹è®Šå•æ³•å’Œè¡¨é”æ–¹å¼ã€‚

åŸå§‹å•é¡Œï¼š{question}
åŸå§‹å›ç­”ï¼š{answer}

è«‹ç”¨ä»¥ä¸‹ JSON æ ¼å¼è¼¸å‡ºï¼š
[
    {{"question": "è®Šé«”å•é¡Œ1", "answer": "è®Šé«”å›ç­”1"}},
    {{"question": "è®Šé«”å•é¡Œ2", "answer": "è®Šé«”å›ç­”2"}},
    ...
]

åªè¼¸å‡º JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼š"""

    response = client.chat.completions.create(
        model="gpt-oss-120b",
        messages=[{"role": "user", "content": prompt}]
    )

    try:
        variations = json.loads(response.choices[0].message.content)
        return variations
    except json.JSONDecodeError:
        print("è§£æå›æ‡‰å¤±æ•—")
        return []


def augment_dataset(seed_data, variations_per_item=2):
    """
    å¢å¼·æ•´å€‹è³‡æ–™é›†

    åƒæ•¸ï¼š
        seed_data: ç¨®å­è³‡æ–™ï¼ˆå•ç­”å°åˆ—è¡¨ï¼‰
        variations_per_item: æ¯ç­†è³‡æ–™ç”Ÿæˆçš„è®Šé«”æ•¸

    å›å‚³ï¼š
        å¢å¼·å¾Œçš„è³‡æ–™é›†
    """
    augmented_data = []

    for item in seed_data:
        # åŠ å…¥åŸå§‹è³‡æ–™
        augmented_data.append(item)

        # ç”Ÿæˆè®Šé«”
        variations = augment_qa_pair(
            item["question"],
            item["answer"],
            variations_per_item
        )

        augmented_data.extend(variations)

    return augmented_data


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    # ç¨®å­è³‡æ–™
    seed_data = [
        {
            "question": "Python çš„ list å’Œ tuple æœ‰ä»€éº¼å·®åˆ¥ï¼Ÿ",
            "answer": "list æ˜¯å¯è®Šçš„ï¼ˆmutableï¼‰ï¼Œå¯ä»¥æ–°å¢ã€ä¿®æ”¹ã€åˆªé™¤å…ƒç´ ï¼›tuple æ˜¯ä¸å¯è®Šçš„ï¼ˆimmutableï¼‰ï¼Œå»ºç«‹å¾Œå°±ä¸èƒ½æ”¹è®Šã€‚tuple çš„æ•ˆèƒ½æ¯” list å¥½ï¼Œé©åˆç”¨æ–¼ä¸éœ€è¦ä¿®æ”¹çš„è³‡æ–™ã€‚"
        },
        {
            "question": "ä»€éº¼æ˜¯è¿´åœˆï¼Ÿ",
            "answer": "è¿´åœˆæ˜¯è®“ç¨‹å¼é‡è¤‡åŸ·è¡ŒæŸæ®µç¨‹å¼ç¢¼çš„çµæ§‹ã€‚Python æœ‰å…©ç¨®è¿´åœˆï¼šfor è¿´åœˆç”¨æ–¼éæ­·åºåˆ—ï¼Œwhile è¿´åœˆç”¨æ–¼æ¢ä»¶åˆ¤æ–·ã€‚è¿´åœˆå¯ä»¥æ¸›å°‘é‡è¤‡çš„ç¨‹å¼ç¢¼ï¼Œæé«˜æ•ˆç‡ã€‚"
        }
    ]

    print("=== è³‡æ–™å¢å¼·ç¤ºç¯„ ===\n")
    print(f"åŸå§‹è³‡æ–™ï¼š{len(seed_data)} ç­†")
    print("-" * 50)

    # å¢å¼·è³‡æ–™
    augmented = augment_dataset(seed_data, variations_per_item=2)

    print(f"\nå¢å¼·å¾Œè³‡æ–™ï¼š{len(augmented)} ç­†")
    print("-" * 50)

    # é¡¯ç¤ºçµæœ
    for i, item in enumerate(augmented, 1):
        print(f"\n[{i}] Q: {item.get('question', 'N/A')[:50]}...")
        print(f"    A: {item.get('answer', 'N/A')[:50]}...")
```

---

### ç¯„ä¾‹ 18ï¼šè©•ä¼° Fine-Tuning æ•ˆæœ

```python
"""
ç¯„ä¾‹ 18ï¼šè©•ä¼°æ¨¡å‹æ•ˆæœ
æ¯”è¼ƒåŸå§‹æ¨¡å‹å’Œå¾®èª¿å¾Œæ¨¡å‹çš„å›ç­”å“è³ª
"""

from openai import OpenAI
import json


client = OpenAI(
    base_url="http://localhost:1234/v1",
    api_key="not-needed"
)


def get_response(model, question, system_prompt=None):
    """
    å–å¾—æ¨¡å‹å›æ‡‰
    """
    messages = []

    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})

    messages.append({"role": "user", "content": question})

    response = client.chat.completions.create(
        model=model,
        messages=messages
    )

    return response.choices[0].message.content


def evaluate_response(question, response, criteria):
    """
    ä½¿ç”¨ AI è©•ä¼°å›ç­”å“è³ª

    åƒæ•¸ï¼š
        question: åŸå§‹å•é¡Œ
        response: æ¨¡å‹å›ç­”
        criteria: è©•ä¼°æ¨™æº–

    å›å‚³ï¼š
        è©•ä¼°çµæœï¼ˆ1-5 åˆ†ï¼‰
    """

    prompt = f"""è«‹è©•ä¼°ä»¥ä¸‹å›ç­”çš„å“è³ªï¼Œçµ¦äºˆ 1-5 åˆ†çš„è©•åˆ†ã€‚

å•é¡Œï¼š{question}

å›ç­”ï¼š{response}

è©•ä¼°æ¨™æº–ï¼š
{criteria}

è«‹ç”¨ä»¥ä¸‹ JSON æ ¼å¼å›ç­”ï¼š
{{"score": <1-5çš„æ•¸å­—>, "reason": "è©•åˆ†åŸå› "}}

åªè¼¸å‡º JSONï¼š"""

    eval_response = client.chat.completions.create(
        model="gpt-oss-120b",
        messages=[{"role": "user", "content": prompt}]
    )

    try:
        result = json.loads(eval_response.choices[0].message.content)
        return result
    except:
        return {"score": 0, "reason": "è©•ä¼°å¤±æ•—"}


def compare_models(test_questions, model1, model2, system_prompt=None):
    """
    æ¯”è¼ƒå…©å€‹æ¨¡å‹çš„è¡¨ç¾
    """
    criteria = """
    - æº–ç¢ºæ€§ï¼šå›ç­”æ˜¯å¦æ­£ç¢º
    - å®Œæ•´æ€§ï¼šæ˜¯å¦æ¶µè“‹é‡è¦è³‡è¨Š
    - æ¸…æ™°åº¦ï¼šæ˜¯å¦å®¹æ˜“ç†è§£
    - å¯¦ç”¨æ€§ï¼šæ˜¯å¦æä¾›æœ‰ç”¨çš„ç¯„ä¾‹æˆ–å»ºè­°
    """

    results = []

    for question in test_questions:
        print(f"\nå•é¡Œï¼š{question}")
        print("-" * 40)

        # å–å¾—å…©å€‹æ¨¡å‹çš„å›ç­”
        response1 = get_response(model1, question, system_prompt)
        response2 = get_response(model2, question, system_prompt)

        # è©•ä¼°å…©å€‹å›ç­”
        eval1 = evaluate_response(question, response1, criteria)
        eval2 = evaluate_response(question, response2, criteria)

        print(f"\næ¨¡å‹ 1 ({model1})ï¼š")
        print(f"  å›ç­”ï¼š{response1[:100]}...")
        print(f"  è©•åˆ†ï¼š{eval1.get('score', 'N/A')}/5")

        print(f"\næ¨¡å‹ 2 ({model2})ï¼š")
        print(f"  å›ç­”ï¼š{response2[:100]}...")
        print(f"  è©•åˆ†ï¼š{eval2.get('score', 'N/A')}/5")

        results.append({
            "question": question,
            "model1_score": eval1.get("score", 0),
            "model2_score": eval2.get("score", 0)
        })

    return results


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    # æ¸¬è©¦å•é¡Œ
    test_questions = [
        "å¦‚ä½•åœ¨ Python ä¸­è®€å– CSV æª”æ¡ˆï¼Ÿ",
        "è§£é‡‹ä»€éº¼æ˜¯ API",
        "for è¿´åœˆå’Œ while è¿´åœˆæœ‰ä»€éº¼å·®åˆ¥ï¼Ÿ"
    ]

    print("=== æ¨¡å‹æ¯”è¼ƒè©•ä¼° ===")

    # æ¯”è¼ƒæœ‰ç„¡ç³»çµ±æç¤ºè©çš„å·®ç•°
    results = compare_models(
        test_questions,
        model1="gpt-oss-120b",
        model2="gpt-oss-120b",
        system_prompt="ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¨‹å¼è¨­è¨ˆæ•™å¸«ï¼Œç”¨ç°¡å–®æ˜“æ‡‚çš„æ–¹å¼å›ç­”å•é¡Œã€‚"
    )

    # çµ±è¨ˆçµæœ
    print("\n" + "=" * 50)
    print("è©•ä¼°ç¸½çµ")
    print("=" * 50)

    avg1 = sum(r["model1_score"] for r in results) / len(results)
    avg2 = sum(r["model2_score"] for r in results) / len(results)

    print(f"æ¨¡å‹ 1 å¹³å‡åˆ†æ•¸ï¼š{avg1:.2f}")
    print(f"æ¨¡å‹ 2 å¹³å‡åˆ†æ•¸ï¼š{avg2:.2f}")
```

---

## Fine-Tuning æˆæ•ˆè©•ä¼°æ–¹æ³•

### ç‚ºä»€éº¼è¦è©•ä¼°ï¼Ÿ

Fine-Tuning å¾Œï¼Œä½ éœ€è¦çŸ¥é“ï¼š
- æ¨¡å‹æœ‰æ²’æœ‰è®Šå¥½ï¼Ÿ
- å¥½äº†å¤šå°‘ï¼Ÿ
- æœ‰æ²’æœ‰å‰¯ä½œç”¨ï¼ˆä¾‹å¦‚å…¶ä»–èƒ½åŠ›è®Šå·®ï¼‰ï¼Ÿ

**ç”Ÿæ´»æ¯”å–»ï¼š** å°±åƒå­¸ç”Ÿè£œç¿’å¾Œè¦è€ƒè©¦ï¼Œçœ‹çœ‹æˆç¸¾æœ‰æ²’æœ‰é€²æ­¥ã€‚

---

### è©•ä¼°æ–¹æ³•ä¸€ï¼šå®šé‡æŒ‡æ¨™ï¼ˆç”¨æ•¸å­—è¡¡é‡ï¼‰

| æŒ‡æ¨™ | èªªæ˜ | æ•¸å€¼æ„ç¾© |
|------|------|----------|
| **Lossï¼ˆæå¤±å€¼ï¼‰** | æ¨¡å‹é æ¸¬èˆ‡æ­£ç¢ºç­”æ¡ˆçš„å·®è· | è¶Šä½è¶Šå¥½ |
| **Perplexityï¼ˆå›°æƒ‘åº¦ï¼‰** | æ¨¡å‹å°æ–‡å­—çš„é æ¸¬ä¿¡å¿ƒ | è¶Šä½è¶Šå¥½ |
| **Accuracyï¼ˆæº–ç¢ºç‡ï¼‰** | æ­£ç¢ºå›ç­”çš„æ¯”ä¾‹ | è¶Šé«˜è¶Šå¥½ |
| **F1 Score** | ç²¾ç¢ºç‡èˆ‡å¬å›ç‡çš„å¹³è¡¡ | è¶Šé«˜è¶Šå¥½ï¼ˆ0-1ï¼‰|
| **BLEU Score** | ç”Ÿæˆæ–‡å­—èˆ‡æ¨™æº–ç­”æ¡ˆçš„ç›¸ä¼¼åº¦ | è¶Šé«˜è¶Šå¥½ï¼ˆ0-100ï¼‰|

---

### è©•ä¼°æ–¹æ³•äºŒï¼šå®šæ€§è©•ä¼°ï¼ˆäººå·¥åˆ¤æ–·ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  äººå·¥è©•ä¼°æª¢æŸ¥æ¸…å–®                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â–¡ å›ç­”æº–ç¢ºæ€§ - è³‡è¨Šæ˜¯å¦æ­£ç¢ºç„¡èª¤ï¼Ÿ                 â”‚
â”‚  â–¡ é¢¨æ ¼ä¸€è‡´æ€§ - èªæ°£å’Œæ ¼å¼æ˜¯å¦ç¬¦åˆæœŸæœ›ï¼Ÿ           â”‚
â”‚  â–¡ ä»»å‹™å®Œæˆåº¦ - æ˜¯å¦å®Œæ•´å›ç­”å•é¡Œï¼Ÿ                 â”‚
â”‚  â–¡ æµæš¢åº¦     - æ–‡å­—æ˜¯å¦è‡ªç„¶é€šé †ï¼Ÿ                 â”‚
â”‚  â–¡ å®‰å…¨æ€§     - æ˜¯å¦é¿å…ä¸ç•¶æˆ–æœ‰å®³å…§å®¹ï¼Ÿ           â”‚
â”‚  â–¡ å‰µé€ æ€§     - å›ç­”æ˜¯å¦æœ‰è¦‹è§£è€Œéæ­»æ¿ï¼Ÿ           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### è©•ä¼°æ–¹æ³•ä¸‰ï¼šA/B æ¸¬è©¦

æ¯”è¼ƒåŸå§‹æ¨¡å‹å’Œ Fine-Tuned æ¨¡å‹çš„è¡¨ç¾ï¼š

```
        ç›¸åŒçš„å•é¡Œ
            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
    â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åŸå§‹   â”‚    â”‚ å¾®èª¿å¾Œ â”‚
â”‚ æ¨¡å‹   â”‚    â”‚ æ¨¡å‹   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚               â”‚
    â–¼               â–¼
  å›ç­” A          å›ç­” B
    â”‚               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
        æ¯”è¼ƒè©•åˆ†
```

---

### ç¯„ä¾‹ 19ï¼šå®Œæ•´çš„æ¨¡å‹è©•ä¼°ç³»çµ±

```python
"""
ç¯„ä¾‹ 19ï¼šå®Œæ•´çš„ Fine-Tuning è©•ä¼°ç³»çµ±
åŒ…å«å¤šç¨®è©•ä¼°æŒ‡æ¨™å’Œè¦–è¦ºåŒ–çµæœ
"""

from openai import OpenAI
import json

client = OpenAI(
    base_url="http://localhost:1234/v1",
    api_key="not-needed"
)


class ModelEvaluator:
    """
    æ¨¡å‹è©•ä¼°å™¨
    ç”¨æ–¼è©•ä¼° Fine-Tuning çš„æ•ˆæœ
    """

    def __init__(self, base_model, finetuned_model=None):
        """
        åˆå§‹åŒ–è©•ä¼°å™¨

        åƒæ•¸ï¼š
            base_model: åŸå§‹æ¨¡å‹åç¨±
            finetuned_model: å¾®èª¿å¾Œæ¨¡å‹åç¨±ï¼ˆå¯é¸ï¼‰
        """
        self.base_model = base_model
        self.finetuned_model = finetuned_model
        self.results = []

    def get_response(self, model, question, system_prompt=None):
        """å–å¾—æ¨¡å‹å›æ‡‰"""
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": question})

        response = client.chat.completions.create(
            model=model,
            messages=messages
        )
        return response.choices[0].message.content

    def score_response(self, question, response, criteria):
        """
        ä½¿ç”¨ AI è©•åˆ†å›ç­”å“è³ªï¼ˆ1-5 åˆ†ï¼‰
        """
        prompt = f"""è«‹è©•ä¼°ä»¥ä¸‹å›ç­”çš„å“è³ªï¼Œé‡å°æ¯å€‹æ¨™æº–çµ¦äºˆ 1-5 åˆ†ã€‚

å•é¡Œï¼š{question}

å›ç­”ï¼š{response}

è©•ä¼°æ¨™æº–ï¼š
{criteria}

è«‹ç”¨ä»¥ä¸‹ JSON æ ¼å¼å›ç­”ï¼ˆåªè¼¸å‡º JSONï¼‰ï¼š
{{
    "accuracy": <1-5>,
    "completeness": <1-5>,
    "clarity": <1-5>,
    "usefulness": <1-5>,
    "overall": <1-5>,
    "comment": "ç°¡çŸ­è©•èª"
}}"""

        eval_response = client.chat.completions.create(
            model=self.base_model,
            messages=[{"role": "user", "content": prompt}]
        )

        try:
            return json.loads(eval_response.choices[0].message.content)
        except:
            return {"overall": 0, "comment": "è©•ä¼°å¤±æ•—"}

    def evaluate_single(self, question, expected_answer=None, system_prompt=None):
        """
        è©•ä¼°å–®ä¸€å•é¡Œ
        """
        criteria = """
        - accuracyï¼ˆæº–ç¢ºæ€§ï¼‰ï¼šè³‡è¨Šæ˜¯å¦æ­£ç¢º
        - completenessï¼ˆå®Œæ•´æ€§ï¼‰ï¼šæ˜¯å¦æ¶µè“‹æ‰€æœ‰é‡é»
        - clarityï¼ˆæ¸…æ™°åº¦ï¼‰ï¼šæ˜¯å¦å®¹æ˜“ç†è§£
        - usefulnessï¼ˆå¯¦ç”¨æ€§ï¼‰ï¼šæ˜¯å¦æœ‰å¹«åŠ©
        - overallï¼ˆæ•´é«”ï¼‰ï¼šç¶œåˆè©•åˆ†
        """

        result = {"question": question}

        # è©•ä¼°åŸå§‹æ¨¡å‹
        base_response = self.get_response(
            self.base_model, question, system_prompt
        )
        base_score = self.score_response(question, base_response, criteria)
        result["base"] = {
            "response": base_response,
            "scores": base_score
        }

        # å¦‚æœæœ‰å¾®èª¿æ¨¡å‹ï¼Œä¹Ÿé€²è¡Œè©•ä¼°
        if self.finetuned_model:
            ft_response = self.get_response(
                self.finetuned_model, question, system_prompt
            )
            ft_score = self.score_response(question, ft_response, criteria)
            result["finetuned"] = {
                "response": ft_response,
                "scores": ft_score
            }

        self.results.append(result)
        return result

    def evaluate_batch(self, questions, system_prompt=None):
        """
        æ‰¹æ¬¡è©•ä¼°å¤šå€‹å•é¡Œ
        """
        print(f"é–‹å§‹è©•ä¼° {len(questions)} å€‹å•é¡Œ...\n")

        for i, q in enumerate(questions, 1):
            print(f"[{i}/{len(questions)}] è©•ä¼°ä¸­: {q[:30]}...")
            self.evaluate_single(q, system_prompt=system_prompt)

        return self.get_summary()

    def get_summary(self):
        """
        å–å¾—è©•ä¼°æ‘˜è¦
        """
        if not self.results:
            return "å°šç„¡è©•ä¼°çµæœ"

        summary = {
            "total_questions": len(self.results),
            "base_model": {
                "avg_overall": 0,
                "avg_accuracy": 0,
                "avg_clarity": 0
            }
        }

        # è¨ˆç®—åŸå§‹æ¨¡å‹å¹³å‡åˆ†æ•¸
        base_scores = [r["base"]["scores"] for r in self.results]
        summary["base_model"]["avg_overall"] = sum(
            s.get("overall", 0) for s in base_scores
        ) / len(base_scores)
        summary["base_model"]["avg_accuracy"] = sum(
            s.get("accuracy", 0) for s in base_scores
        ) / len(base_scores)
        summary["base_model"]["avg_clarity"] = sum(
            s.get("clarity", 0) for s in base_scores
        ) / len(base_scores)

        # å¦‚æœæœ‰å¾®èª¿æ¨¡å‹çš„çµæœ
        if self.finetuned_model and "finetuned" in self.results[0]:
            summary["finetuned_model"] = {"avg_overall": 0, "avg_accuracy": 0, "avg_clarity": 0}
            ft_scores = [r["finetuned"]["scores"] for r in self.results]
            summary["finetuned_model"]["avg_overall"] = sum(
                s.get("overall", 0) for s in ft_scores
            ) / len(ft_scores)
            summary["finetuned_model"]["avg_accuracy"] = sum(
                s.get("accuracy", 0) for s in ft_scores
            ) / len(ft_scores)
            summary["finetuned_model"]["avg_clarity"] = sum(
                s.get("clarity", 0) for s in ft_scores
            ) / len(ft_scores)

            # è¨ˆç®—æ”¹å–„å¹…åº¦
            summary["improvement"] = {
                "overall": summary["finetuned_model"]["avg_overall"] - summary["base_model"]["avg_overall"],
                "accuracy": summary["finetuned_model"]["avg_accuracy"] - summary["base_model"]["avg_accuracy"],
            }

        return summary

    def print_report(self):
        """
        å°å‡ºè©•ä¼°å ±å‘Š
        """
        summary = self.get_summary()

        print("\n" + "=" * 60)
        print("ğŸ“Š Fine-Tuning è©•ä¼°å ±å‘Š")
        print("=" * 60)

        print(f"\nğŸ“ è©•ä¼°å•é¡Œæ•¸ï¼š{summary['total_questions']}")

        print(f"\nğŸ”µ åŸå§‹æ¨¡å‹ ({self.base_model})ï¼š")
        print(f"   æ•´é«”è©•åˆ†ï¼š{summary['base_model']['avg_overall']:.2f}/5")
        print(f"   æº–ç¢ºæ€§ï¼š  {summary['base_model']['avg_accuracy']:.2f}/5")
        print(f"   æ¸…æ™°åº¦ï¼š  {summary['base_model']['avg_clarity']:.2f}/5")

        if "finetuned_model" in summary:
            print(f"\nğŸŸ¢ å¾®èª¿æ¨¡å‹ ({self.finetuned_model})ï¼š")
            print(f"   æ•´é«”è©•åˆ†ï¼š{summary['finetuned_model']['avg_overall']:.2f}/5")
            print(f"   æº–ç¢ºæ€§ï¼š  {summary['finetuned_model']['avg_accuracy']:.2f}/5")
            print(f"   æ¸…æ™°åº¦ï¼š  {summary['finetuned_model']['avg_clarity']:.2f}/5")

            print(f"\nğŸ“ˆ æ”¹å–„å¹…åº¦ï¼š")
            imp = summary["improvement"]
            overall_pct = (imp["overall"] / summary["base_model"]["avg_overall"]) * 100 if summary["base_model"]["avg_overall"] > 0 else 0
            print(f"   æ•´é«”ï¼š{imp['overall']:+.2f} ({overall_pct:+.1f}%)")
            print(f"   æº–ç¢ºæ€§ï¼š{imp['accuracy']:+.2f}")

        print("\n" + "=" * 60)


# ä¸»ç¨‹å¼
if __name__ == "__main__":
    # å»ºç«‹è©•ä¼°å™¨
    evaluator = ModelEvaluator(
        base_model="gpt-oss-120b",
        # finetuned_model="my-finetuned-model"  # å¦‚æœæœ‰å¾®èª¿æ¨¡å‹
    )

    # æ¸¬è©¦å•é¡Œé›†
    test_questions = [
        "ä»€éº¼æ˜¯è®Šæ•¸ï¼Ÿè«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ã€‚",
        "Python ä¸­ list å’Œ dictionary æœ‰ä»€éº¼å·®åˆ¥ï¼Ÿ",
        "å¦‚ä½•è™•ç†ç¨‹å¼ä¸­çš„éŒ¯èª¤ï¼Ÿ",
        "ä»€éº¼æ˜¯éè¿´ï¼Ÿè«‹èˆ‰ä¾‹èªªæ˜ã€‚",
        "è§£é‡‹ä»€éº¼æ˜¯ APIï¼Œä»¥åŠç‚ºä»€éº¼è¦ç”¨å®ƒã€‚"
    ]

    # åŸ·è¡Œè©•ä¼°
    evaluator.evaluate_batch(
        test_questions,
        system_prompt="ä½ æ˜¯ä¸€ä½ç¨‹å¼è¨­è¨ˆæ•™å¸«ï¼Œç”¨ç°¡å–®çš„æ–¹å¼å›ç­”å•é¡Œã€‚"
    )

    # å°å‡ºå ±å‘Š
    evaluator.print_report()
```

---

### ç›£æ§è¨“ç·´éç¨‹

è¨“ç·´ Fine-Tuning æ™‚ï¼Œè¦æ³¨æ„é€™äº›è­¦å‘Šä¿¡è™Ÿï¼š

```
âœ… æ­£å¸¸æƒ…æ³ï¼š
   Training Loss:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ â†’ é€æ¼¸ä¸‹é™
   Validation Loss:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ â†’ è·Ÿè‘—ä¸‹é™

âš ï¸ éæ“¬åˆ (Overfitting)ï¼š
   Training Loss:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â†’ æŒçºŒä¸‹é™
   Validation Loss:  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ â†’ é–‹å§‹ä¸Šå‡ âŒ

   ç—‡ç‹€ï¼šè¨“ç·´è³‡æ–™è¡¨ç¾å¥½ï¼Œæ–°è³‡æ–™è¡¨ç¾å·®
   è§£æ³•ï¼šEarly Stoppingã€å¢åŠ è³‡æ–™ã€Dropout

âš ï¸ æ¬ æ“¬åˆ (Underfitting)ï¼š
   Training Loss:    â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ â†’ ä¸‹é™ç·©æ…¢
   Validation Loss:  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ â†’ éƒ½å¾ˆé«˜ âŒ

   ç—‡ç‹€ï¼šè¨“ç·´å’Œæ¸¬è©¦éƒ½è¡¨ç¾ä¸å¥½
   è§£æ³•ï¼šå¢åŠ è¨“ç·´æ™‚é–“ã€èª¿æ•´å­¸ç¿’ç‡
```

---

### è©•ä¼°æª¢æŸ¥æ¸…å–®

åœ¨å®£å¸ƒ Fine-Tuning æˆåŠŸå‰ï¼Œç¢ºèªä»¥ä¸‹é …ç›®ï¼š

```
â–¡ 1. æ¸¬è©¦é›†ç¨ç«‹
     â””â”€â”€ æ¸¬è©¦è³‡æ–™æ²’æœ‰åƒèˆ‡è¨“ç·´

â–¡ 2. åŸºæº–ç·šæ¯”è¼ƒ
     â””â”€â”€ èˆ‡åŸå§‹æ¨¡å‹æ¯”è¼ƒï¼Œç¢ºèªæœ‰æ”¹å–„

â–¡ 3. å¤šç¶­åº¦è©•ä¼°
     â””â”€â”€ ä¸åªçœ‹ä¸€å€‹æŒ‡æ¨™ï¼Œç¶œåˆè©•ä¼°

â–¡ 4. é‚Šç•Œæ¡ˆä¾‹æ¸¬è©¦
     â””â”€â”€ æ¸¬è©¦å›°é›£ã€ç‰¹æ®Šçš„å•é¡Œ

â–¡ 5. äººå·¥æŠ½æŸ¥
     â””â”€â”€ éš¨æ©ŸæŠ½ 50-100 å€‹å›ç­”äººå·¥å¯©æ ¸

â–¡ 6. å‰¯ä½œç”¨æª¢æŸ¥
     â””â”€â”€ ç¢ºèªå…¶ä»–èƒ½åŠ›æ²’æœ‰é€€æ­¥

â–¡ 7. å¯¦éš›å ´æ™¯æ¸¬è©¦
     â””â”€â”€ åœ¨çœŸå¯¦ä½¿ç”¨æƒ…å¢ƒä¸­æ¸¬è©¦
```

---

### å¸¸è¦‹è©•ä¼°éŒ¯èª¤

| éŒ¯èª¤ | å•é¡Œ | æ­£ç¢ºåšæ³• |
|------|------|----------|
| ç”¨è¨“ç·´è³‡æ–™æ¸¬è©¦ | æœƒå¾—åˆ°è™›å‡çš„å¥½æˆç¸¾ | ä¿ç•™ 20% è³‡æ–™ä½œæ¸¬è©¦ |
| åªçœ‹å–®ä¸€æŒ‡æ¨™ | è©•ä¼°ç‰‡é¢ä¸å®Œæ•´ | å¤šæŒ‡æ¨™ç¶œåˆè©•ä¼° |
| å¿½ç•¥é‚Šç•Œæ¡ˆä¾‹ | æ¼æ‰æ¥µç«¯æƒ…æ³ | åŠ å…¥å›°é›£æ¸¬è©¦æ¡ˆä¾‹ |
| ä¸è¨­åŸºæº–ç·š | ç„¡æ³•é‡åŒ–æ”¹å–„ç¨‹åº¦ | å…ˆæ¸¬åŸå§‹æ¨¡å‹è¡¨ç¾ |
| æ¨£æœ¬å¤ªå°‘ | çµæœä¸å¯é  | è‡³å°‘ 50-100 å€‹æ¸¬è©¦æ¨£æœ¬ |

---

## å»¶ä¼¸å­¸ç¿’

1. **Ollama å®˜æ–¹æ–‡ä»¶**ï¼šhttps://ollama.com
2. **LM Studio å®˜æ–¹ç¶²ç«™**ï¼šhttps://lmstudio.ai
3. **OpenAI Python SDK**ï¼šhttps://github.com/openai/openai-python
4. **Python requests æ•™å­¸**ï¼šhttps://docs.python-requests.org
5. **JSON æ ¼å¼ä»‹ç´¹**ï¼šhttps://www.json.org

---

## æˆæ¬Š

æœ¬æ•™å­¸å…§å®¹æ¡ç”¨ MIT æˆæ¬Šï¼Œæ­¡è¿è‡ªç”±ä½¿ç”¨èˆ‡ä¿®æ”¹ã€‚
