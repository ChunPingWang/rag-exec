{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 範例 18：評估模型效果\n",
    "\n",
    "比較原始模型和微調後模型的回答品質！\n",
    "\n",
    "## 為什麼需要評估？\n",
    "\n",
    "Fine-Tuning 後需要確認：\n",
    "- 模型是否真的變好了？\n",
    "- 哪些方面改善了？\n",
    "- 有沒有產生新問題？\n",
    "\n",
    "## 學習目標\n",
    "- 了解模型評估的方法\n",
    "- 學會使用 AI 評分\n",
    "- 建立評估流程\n",
    "\n",
    "## 前置需求\n",
    "- LM Studio 運行中，Local Server 已啟動\n",
    "- 安裝 openai 套件：`pip install openai`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: 匯入套件並設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"not-needed\"\n",
    ")\n",
    "\n",
    "print(\"已連接到 LM Studio！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: 定義評估用的測試問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試問題集\n",
    "test_questions = [\n",
    "    \"如何在 Python 中讀取 CSV 檔案？\",\n",
    "    \"解釋什麼是 API\",\n",
    "    \"for 迴圈和 while 迴圈有什麼差別？\"\n",
    "]\n",
    "\n",
    "print(\"測試問題：\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: 建立回應取得函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(model, question, system_prompt=None):\n",
    "    \"\"\"\n",
    "    取得模型回應\n",
    "    \n",
    "    參數：\n",
    "        model: 模型名稱\n",
    "        question: 問題\n",
    "        system_prompt: 系統提示詞（可選）\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: 建立 AI 評分函數\n",
    "\n",
    "使用 AI 來評估另一個 AI 的回答品質（這是一種常見的評估方法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(question, response, criteria):\n",
    "    \"\"\"\n",
    "    使用 AI 評估回答品質\n",
    "\n",
    "    參數：\n",
    "        question: 原始問題\n",
    "        response: 模型回答\n",
    "        criteria: 評估標準\n",
    "\n",
    "    回傳：\n",
    "        評估結果（分數和原因）\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"請評估以下回答的品質，給予 1-5 分的評分。\n",
    "\n",
    "問題：{question}\n",
    "\n",
    "回答：{response}\n",
    "\n",
    "評估標準：\n",
    "{criteria}\n",
    "\n",
    "評分說明：\n",
    "- 5 分：優秀，完全符合所有標準\n",
    "- 4 分：良好，大部分符合標準\n",
    "- 3 分：普通，基本符合標準\n",
    "- 2 分：不佳，有明顯問題\n",
    "- 1 分：很差，完全不符合標準\n",
    "\n",
    "請用以下 JSON 格式回答：\n",
    "{{\"score\": <1-5的數字>, \"reason\": \"評分原因（一句話）\"}}\n",
    "\n",
    "只輸出 JSON：\"\"\"\n",
    "\n",
    "    eval_response = client.chat.completions.create(\n",
    "        model=\"gpt-oss-120b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        result = eval_response.choices[0].message.content\n",
    "        # 找出 JSON\n",
    "        start = result.find('{')\n",
    "        end = result.rfind('}') + 1\n",
    "        if start != -1 and end != 0:\n",
    "            return json.loads(result[start:end])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return {\"score\": 0, \"reason\": \"評估失敗\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: 測試單一問題評估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評估標準\n",
    "evaluation_criteria = \"\"\"\n",
    "- 準確性：回答是否正確\n",
    "- 完整性：是否涵蓋重要資訊\n",
    "- 清晰度：是否容易理解\n",
    "- 實用性：是否提供有用的範例或建議\n",
    "\"\"\"\n",
    "\n",
    "# 測試\n",
    "test_question = test_questions[0]\n",
    "print(f\"問題：{test_question}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 取得回答\n",
    "response = get_response(\"gpt-oss-120b\", test_question)\n",
    "print(f\"回答：{response[:200]}...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 評估\n",
    "evaluation = evaluate_response(test_question, response, evaluation_criteria)\n",
    "print(f\"評分：{evaluation.get('score', 'N/A')}/5\")\n",
    "print(f\"原因：{evaluation.get('reason', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: 比較有無系統提示詞的差異"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_without_system_prompt(question):\n",
    "    \"\"\"\n",
    "    比較有無系統提示詞的回答差異\n",
    "    \"\"\"\n",
    "    system_prompt = \"你是一位專業的程式設計教師，用簡單易懂的方式回答問題，並提供程式碼範例。\"\n",
    "    \n",
    "    # 無系統提示詞\n",
    "    response_no_system = get_response(\"gpt-oss-120b\", question)\n",
    "    eval_no_system = evaluate_response(question, response_no_system, evaluation_criteria)\n",
    "    \n",
    "    # 有系統提示詞\n",
    "    response_with_system = get_response(\"gpt-oss-120b\", question, system_prompt)\n",
    "    eval_with_system = evaluate_response(question, response_with_system, evaluation_criteria)\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"no_system\": {\n",
    "            \"response\": response_no_system,\n",
    "            \"score\": eval_no_system.get(\"score\", 0),\n",
    "            \"reason\": eval_no_system.get(\"reason\", \"\")\n",
    "        },\n",
    "        \"with_system\": {\n",
    "            \"response\": response_with_system,\n",
    "            \"score\": eval_with_system.get(\"score\", 0),\n",
    "            \"reason\": eval_with_system.get(\"reason\", \"\")\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較測試\n",
    "print(\"=== 比較有無系統提示詞 ===\")\n",
    "comparison = compare_with_without_system_prompt(test_questions[1])\n",
    "\n",
    "print(f\"\\n問題：{comparison['question']}\")\n",
    "print(\"\\n--- 無系統提示詞 ---\")\n",
    "print(f\"評分：{comparison['no_system']['score']}/5\")\n",
    "print(f\"回答：{comparison['no_system']['response'][:150]}...\")\n",
    "\n",
    "print(\"\\n--- 有系統提示詞 ---\")\n",
    "print(f\"評分：{comparison['with_system']['score']}/5\")\n",
    "print(f\"回答：{comparison['with_system']['response'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: 批次評估多個問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_evaluate(questions, system_prompt=None):\n",
    "    \"\"\"\n",
    "    批次評估多個問題\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\n評估第 {i}/{len(questions)} 題...\")\n",
    "        \n",
    "        # 取得回答\n",
    "        response = get_response(\"gpt-oss-120b\", question, system_prompt)\n",
    "        \n",
    "        # 評估\n",
    "        evaluation = evaluate_response(question, response, evaluation_criteria)\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"response\": response,\n",
    "            \"score\": evaluation.get(\"score\", 0),\n",
    "            \"reason\": evaluation.get(\"reason\", \"\")\n",
    "        })\n",
    "        \n",
    "        print(f\"  評分：{evaluation.get('score', 'N/A')}/5\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 執行批次評估\n",
    "print(\"=== 批次評估（有系統提示詞）===\")\n",
    "system_prompt = \"你是一位程式設計教師，用簡單易懂的方式回答問題。\"\n",
    "\n",
    "results = batch_evaluate(test_questions, system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: 統計與報告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(results):\n",
    "    \"\"\"\n",
    "    生成評估報告\n",
    "    \"\"\"\n",
    "    scores = [r[\"score\"] for r in results if r[\"score\"] > 0]\n",
    "    \n",
    "    if not scores:\n",
    "        print(\"沒有有效的評估結果\")\n",
    "        return\n",
    "    \n",
    "    avg_score = sum(scores) / len(scores)\n",
    "    max_score = max(scores)\n",
    "    min_score = min(scores)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"評估報告\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\n評估題數：{len(results)}\")\n",
    "    print(f\"平均分數：{avg_score:.2f}/5\")\n",
    "    print(f\"最高分數：{max_score}/5\")\n",
    "    print(f\"最低分數：{min_score}/5\")\n",
    "    \n",
    "    # 分數分布\n",
    "    print(\"\\n分數分布：\")\n",
    "    for score in range(5, 0, -1):\n",
    "        count = scores.count(score)\n",
    "        bar = \"█\" * count\n",
    "        print(f\"  {score} 分：{bar} ({count})\")\n",
    "    \n",
    "    # 詳細結果\n",
    "    print(\"\\n詳細結果：\")\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"\\n[{i}] {r['question'][:30]}...\")\n",
    "        print(f\"    分數：{r['score']}/5\")\n",
    "        print(f\"    評語：{r['reason']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成報告\n",
    "generate_report(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評估指標說明\n",
    "\n",
    "### 常用評估維度\n",
    "\n",
    "| 維度 | 說明 |\n",
    "|------|------|\n",
    "| 準確性 | 資訊是否正確 |\n",
    "| 完整性 | 是否涵蓋所有重點 |\n",
    "| 清晰度 | 是否容易理解 |\n",
    "| 實用性 | 是否有幫助 |\n",
    "| 相關性 | 是否切題 |\n",
    "\n",
    "### 評估方法比較\n",
    "\n",
    "| 方法 | 優點 | 缺點 |\n",
    "|------|------|------|\n",
    "| 人工評估 | 最準確 | 耗時、成本高 |\n",
    "| AI 評估 | 快速、一致 | 可能有偏差 |\n",
    "| 自動指標 | 客觀、快速 | 不一定反映品質 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重點回顧\n",
    "\n",
    "1. **評估的重要性**：確認模型是否真正改善\n",
    "\n",
    "2. **評估方法**：\n",
    "   - AI 評分（本範例）\n",
    "   - 人工評估\n",
    "   - 自動指標\n",
    "\n",
    "3. **比較方式**：\n",
    "   - 有無系統提示詞\n",
    "   - 不同模型\n",
    "   - Fine-Tuning 前後\n",
    "\n",
    "4. **注意事項**：\n",
    "   - 使用多個問題測試\n",
    "   - 考慮多個評估維度\n",
    "   - AI 評估可能有偏差\n",
    "\n",
    "## 下一步\n",
    "\n",
    "在最後一個範例中，我們將建立一個完整的評估系統！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
